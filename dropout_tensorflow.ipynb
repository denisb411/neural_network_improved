{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util import get_normalized_data\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, M1, M2):\n",
    "        self.M1 = M1\n",
    "        self.M2 = M2\n",
    "        W = np.random.randn(M1, M2) * np.sqrt(2.0 / M1)\n",
    "        b = np.zeros(M2)\n",
    "        self.W = tf.Variable(W.astype(np.float32))\n",
    "        self.b = tf.Variable(b.astype(np.float32))\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def forward(self, X):\n",
    "        return tf.nn.relu(tf.matmul(X, self.W) + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ANN(object):\n",
    "    def __init__(self, hidden_layer_sizes, p_keep):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.dropout_rates = p_keep\n",
    "\n",
    "    def fit(self, X, Y, lr=1e-4, mu=0.9, decay=0.9, epochs=15, batch_sz=100, split=True, print_every=20):\n",
    "        # make a validation set\n",
    "        X, Y = shuffle(X, Y)\n",
    "        X = X.astype(np.float32)\n",
    "        Y = Y.astype(np.int64)\n",
    "        if split:\n",
    "            Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "            X, Y = X[:-1000], Y[:-1000]\n",
    "        else:\n",
    "            Xvalid, Yvalid = X, Y\n",
    "\n",
    "        # initialize hidden layers\n",
    "        N, D = X.shape\n",
    "        K = len(set(Y))\n",
    "        self.hidden_layers = []\n",
    "        M1 = D\n",
    "        for M2 in self.hidden_layer_sizes:\n",
    "            h = HiddenLayer(M1, M2)\n",
    "            self.hidden_layers.append(h)\n",
    "            M1 = M2\n",
    "        W = np.random.randn(M1, K) * np.sqrt(2.0 / M1)\n",
    "        b = np.zeros(K)\n",
    "        self.W = tf.Variable(W.astype(np.float32))\n",
    "        self.b = tf.Variable(b.astype(np.float32))\n",
    "\n",
    "        # collect params for later use\n",
    "        self.params = [self.W, self.b]\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += h.params\n",
    "\n",
    "        # set up tensorflow functions and variables\n",
    "        inputs = tf.placeholder(tf.float32, shape=(None, D), name='inputs')\n",
    "        labels = tf.placeholder(tf.int64, shape=(None,), name='labels')\n",
    "        logits = self.forward(inputs)\n",
    "\n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=logits,\n",
    "                labels=labels\n",
    "            )\n",
    "        )\n",
    "        train_op = tf.train.RMSPropOptimizer(lr, decay=decay, momentum=mu).minimize(cost)\n",
    "        # train_op = tf.train.MomentumOptimizer(lr, momentum=mu).minimize(cost)\n",
    "        # train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "        prediction = self.predict(inputs)\n",
    "\n",
    "        # validation cost will be calculated separately since nothing will be dropped\n",
    "        test_logits = self.forward_test(inputs)\n",
    "        test_cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=test_logits,\n",
    "                labels=labels\n",
    "            )\n",
    "        )\n",
    "\n",
    "        n_batches = N // batch_sz\n",
    "        costs = []\n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            for i in range(epochs):\n",
    "                print(\"epoch:\", i, \"n_batches:\", n_batches)\n",
    "                X, Y = shuffle(X, Y)\n",
    "                for j in range(n_batches):\n",
    "                    Xbatch = X[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "                    Ybatch = Y[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "\n",
    "                    session.run(train_op, feed_dict={inputs: Xbatch, labels: Ybatch})\n",
    "\n",
    "                    if j % print_every == 0:\n",
    "                        c = session.run(test_cost, feed_dict={inputs: Xvalid, labels: Yvalid})\n",
    "                        p = session.run(prediction, feed_dict={inputs: Xvalid})\n",
    "                        costs.append(c)\n",
    "                        e = error_rate(Yvalid, p)\n",
    "                        print(\"i:\", i, \"j:\", j, \"nb:\", n_batches, \"cost:\", c, \"error rate:\", e)\n",
    "        \n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # tf.nn.dropout scales inputs by 1/p_keep\n",
    "        # therefore, during test time, we don't have to scale anything\n",
    "        Z = X\n",
    "        Z = tf.nn.dropout(Z, self.dropout_rates[0])\n",
    "        for h, p in zip(self.hidden_layers, self.dropout_rates[1:]):\n",
    "            Z = h.forward(Z)\n",
    "            Z = tf.nn.dropout(Z, p)\n",
    "        return tf.matmul(Z, self.W) + self.b\n",
    "\n",
    "    def forward_test(self, X):\n",
    "        Z = X\n",
    "        for h in self.hidden_layers:\n",
    "            Z = h.forward(Z)\n",
    "        return tf.matmul(Z, self.W) + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        pY = self.forward_test(X)\n",
    "        return tf.argmax(pY, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_rate(p, t):\n",
    "    return np.mean(p != t)\n",
    "\n",
    "\n",
    "def relu(a):\n",
    "    return a * (a > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in and transforming data...\n",
      "epoch: 0 n_batches: 410\n",
      "i: 0 j: 0 nb: 410 cost: 2.82721 error rate: 0.894\n",
      "i: 0 j: 20 nb: 410 cost: 2.61521 error rate: 0.869\n",
      "i: 0 j: 40 nb: 410 cost: 2.15485 error rate: 0.703\n",
      "i: 0 j: 60 nb: 410 cost: 1.55331 error rate: 0.448\n",
      "i: 0 j: 80 nb: 410 cost: 1.00549 error rate: 0.279\n",
      "i: 0 j: 100 nb: 410 cost: 0.685967 error rate: 0.195\n",
      "i: 0 j: 120 nb: 410 cost: 0.521731 error rate: 0.16\n",
      "i: 0 j: 140 nb: 410 cost: 0.42368 error rate: 0.121\n",
      "i: 0 j: 160 nb: 410 cost: 0.371012 error rate: 0.11\n",
      "i: 0 j: 180 nb: 410 cost: 0.3457 error rate: 0.101\n",
      "i: 0 j: 200 nb: 410 cost: 0.318655 error rate: 0.098\n",
      "i: 0 j: 220 nb: 410 cost: 0.30196 error rate: 0.091\n",
      "i: 0 j: 240 nb: 410 cost: 0.271124 error rate: 0.086\n",
      "i: 0 j: 260 nb: 410 cost: 0.256036 error rate: 0.087\n",
      "i: 0 j: 280 nb: 410 cost: 0.243512 error rate: 0.078\n",
      "i: 0 j: 300 nb: 410 cost: 0.231277 error rate: 0.075\n",
      "i: 0 j: 320 nb: 410 cost: 0.222136 error rate: 0.066\n",
      "i: 0 j: 340 nb: 410 cost: 0.220109 error rate: 0.067\n",
      "i: 0 j: 360 nb: 410 cost: 0.212473 error rate: 0.066\n",
      "i: 0 j: 380 nb: 410 cost: 0.206836 error rate: 0.07\n",
      "i: 0 j: 400 nb: 410 cost: 0.193896 error rate: 0.061\n",
      "epoch: 1 n_batches: 410\n",
      "i: 1 j: 0 nb: 410 cost: 0.192381 error rate: 0.058\n",
      "i: 1 j: 20 nb: 410 cost: 0.193078 error rate: 0.056\n",
      "i: 1 j: 40 nb: 410 cost: 0.186189 error rate: 0.055\n",
      "i: 1 j: 60 nb: 410 cost: 0.176014 error rate: 0.055\n",
      "i: 1 j: 80 nb: 410 cost: 0.17745 error rate: 0.054\n",
      "i: 1 j: 100 nb: 410 cost: 0.174591 error rate: 0.056\n",
      "i: 1 j: 120 nb: 410 cost: 0.171133 error rate: 0.055\n",
      "i: 1 j: 140 nb: 410 cost: 0.168595 error rate: 0.049\n",
      "i: 1 j: 160 nb: 410 cost: 0.166522 error rate: 0.053\n",
      "i: 1 j: 180 nb: 410 cost: 0.175376 error rate: 0.053\n",
      "i: 1 j: 200 nb: 410 cost: 0.171423 error rate: 0.055\n",
      "i: 1 j: 220 nb: 410 cost: 0.170719 error rate: 0.057\n",
      "i: 1 j: 240 nb: 410 cost: 0.163722 error rate: 0.05\n",
      "i: 1 j: 260 nb: 410 cost: 0.163283 error rate: 0.051\n",
      "i: 1 j: 280 nb: 410 cost: 0.152252 error rate: 0.051\n",
      "i: 1 j: 300 nb: 410 cost: 0.15086 error rate: 0.048\n",
      "i: 1 j: 320 nb: 410 cost: 0.156083 error rate: 0.049\n",
      "i: 1 j: 340 nb: 410 cost: 0.157918 error rate: 0.055\n",
      "i: 1 j: 360 nb: 410 cost: 0.157488 error rate: 0.052\n",
      "i: 1 j: 380 nb: 410 cost: 0.157751 error rate: 0.055\n",
      "i: 1 j: 400 nb: 410 cost: 0.157701 error rate: 0.052\n",
      "epoch: 2 n_batches: 410\n",
      "i: 2 j: 0 nb: 410 cost: 0.155963 error rate: 0.053\n",
      "i: 2 j: 20 nb: 410 cost: 0.160281 error rate: 0.053\n",
      "i: 2 j: 40 nb: 410 cost: 0.159436 error rate: 0.052\n",
      "i: 2 j: 60 nb: 410 cost: 0.161469 error rate: 0.052\n",
      "i: 2 j: 80 nb: 410 cost: 0.158368 error rate: 0.053\n",
      "i: 2 j: 100 nb: 410 cost: 0.160962 error rate: 0.053\n",
      "i: 2 j: 120 nb: 410 cost: 0.150397 error rate: 0.055\n",
      "i: 2 j: 140 nb: 410 cost: 0.154239 error rate: 0.052\n",
      "i: 2 j: 160 nb: 410 cost: 0.154261 error rate: 0.047\n",
      "i: 2 j: 180 nb: 410 cost: 0.145082 error rate: 0.045\n",
      "i: 2 j: 200 nb: 410 cost: 0.144483 error rate: 0.045\n",
      "i: 2 j: 220 nb: 410 cost: 0.137613 error rate: 0.039\n",
      "i: 2 j: 240 nb: 410 cost: 0.142224 error rate: 0.046\n",
      "i: 2 j: 260 nb: 410 cost: 0.132925 error rate: 0.041\n",
      "i: 2 j: 280 nb: 410 cost: 0.12231 error rate: 0.04\n",
      "i: 2 j: 300 nb: 410 cost: 0.129499 error rate: 0.041\n",
      "i: 2 j: 320 nb: 410 cost: 0.128437 error rate: 0.039\n",
      "i: 2 j: 340 nb: 410 cost: 0.132842 error rate: 0.044\n",
      "i: 2 j: 360 nb: 410 cost: 0.125564 error rate: 0.041\n",
      "i: 2 j: 380 nb: 410 cost: 0.1287 error rate: 0.039\n",
      "i: 2 j: 400 nb: 410 cost: 0.126881 error rate: 0.04\n",
      "epoch: 3 n_batches: 410\n",
      "i: 3 j: 0 nb: 410 cost: 0.12639 error rate: 0.037\n",
      "i: 3 j: 20 nb: 410 cost: 0.128323 error rate: 0.041\n",
      "i: 3 j: 40 nb: 410 cost: 0.129289 error rate: 0.045\n",
      "i: 3 j: 60 nb: 410 cost: 0.121289 error rate: 0.04\n",
      "i: 3 j: 80 nb: 410 cost: 0.123111 error rate: 0.04\n",
      "i: 3 j: 100 nb: 410 cost: 0.123507 error rate: 0.041\n",
      "i: 3 j: 120 nb: 410 cost: 0.118248 error rate: 0.039\n",
      "i: 3 j: 140 nb: 410 cost: 0.126857 error rate: 0.037\n",
      "i: 3 j: 160 nb: 410 cost: 0.134545 error rate: 0.04\n",
      "i: 3 j: 180 nb: 410 cost: 0.126466 error rate: 0.04\n",
      "i: 3 j: 200 nb: 410 cost: 0.129061 error rate: 0.044\n",
      "i: 3 j: 220 nb: 410 cost: 0.132613 error rate: 0.044\n",
      "i: 3 j: 240 nb: 410 cost: 0.129382 error rate: 0.045\n",
      "i: 3 j: 260 nb: 410 cost: 0.124104 error rate: 0.046\n",
      "i: 3 j: 280 nb: 410 cost: 0.121497 error rate: 0.04\n",
      "i: 3 j: 300 nb: 410 cost: 0.129014 error rate: 0.04\n",
      "i: 3 j: 320 nb: 410 cost: 0.126255 error rate: 0.043\n",
      "i: 3 j: 340 nb: 410 cost: 0.124126 error rate: 0.042\n",
      "i: 3 j: 360 nb: 410 cost: 0.124418 error rate: 0.04\n",
      "i: 3 j: 380 nb: 410 cost: 0.124334 error rate: 0.043\n",
      "i: 3 j: 400 nb: 410 cost: 0.120574 error rate: 0.04\n",
      "epoch: 4 n_batches: 410\n",
      "i: 4 j: 0 nb: 410 cost: 0.123304 error rate: 0.041\n",
      "i: 4 j: 20 nb: 410 cost: 0.122396 error rate: 0.041\n",
      "i: 4 j: 40 nb: 410 cost: 0.11379 error rate: 0.036\n",
      "i: 4 j: 60 nb: 410 cost: 0.111671 error rate: 0.042\n",
      "i: 4 j: 80 nb: 410 cost: 0.122745 error rate: 0.044\n",
      "i: 4 j: 100 nb: 410 cost: 0.127178 error rate: 0.039\n",
      "i: 4 j: 120 nb: 410 cost: 0.132435 error rate: 0.042\n",
      "i: 4 j: 140 nb: 410 cost: 0.137444 error rate: 0.043\n",
      "i: 4 j: 160 nb: 410 cost: 0.136494 error rate: 0.045\n",
      "i: 4 j: 180 nb: 410 cost: 0.128368 error rate: 0.043\n",
      "i: 4 j: 200 nb: 410 cost: 0.123847 error rate: 0.042\n",
      "i: 4 j: 220 nb: 410 cost: 0.122462 error rate: 0.041\n",
      "i: 4 j: 240 nb: 410 cost: 0.12312 error rate: 0.042\n",
      "i: 4 j: 260 nb: 410 cost: 0.124119 error rate: 0.046\n",
      "i: 4 j: 280 nb: 410 cost: 0.113914 error rate: 0.037\n",
      "i: 4 j: 300 nb: 410 cost: 0.115894 error rate: 0.04\n",
      "i: 4 j: 320 nb: 410 cost: 0.116853 error rate: 0.039\n",
      "i: 4 j: 340 nb: 410 cost: 0.123038 error rate: 0.039\n",
      "i: 4 j: 360 nb: 410 cost: 0.125554 error rate: 0.039\n",
      "i: 4 j: 380 nb: 410 cost: 0.127028 error rate: 0.044\n",
      "i: 4 j: 400 nb: 410 cost: 0.131185 error rate: 0.038\n",
      "epoch: 5 n_batches: 410\n",
      "i: 5 j: 0 nb: 410 cost: 0.130148 error rate: 0.038\n",
      "i: 5 j: 20 nb: 410 cost: 0.125018 error rate: 0.037\n",
      "i: 5 j: 40 nb: 410 cost: 0.114708 error rate: 0.035\n",
      "i: 5 j: 60 nb: 410 cost: 0.119817 error rate: 0.039\n",
      "i: 5 j: 80 nb: 410 cost: 0.125877 error rate: 0.04\n",
      "i: 5 j: 100 nb: 410 cost: 0.124468 error rate: 0.037\n",
      "i: 5 j: 120 nb: 410 cost: 0.1234 error rate: 0.039\n",
      "i: 5 j: 140 nb: 410 cost: 0.124158 error rate: 0.041\n",
      "i: 5 j: 160 nb: 410 cost: 0.124832 error rate: 0.041\n",
      "i: 5 j: 180 nb: 410 cost: 0.117104 error rate: 0.034\n",
      "i: 5 j: 200 nb: 410 cost: 0.115309 error rate: 0.036\n",
      "i: 5 j: 220 nb: 410 cost: 0.114092 error rate: 0.038\n",
      "i: 5 j: 240 nb: 410 cost: 0.120145 error rate: 0.04\n",
      "i: 5 j: 260 nb: 410 cost: 0.127995 error rate: 0.039\n",
      "i: 5 j: 280 nb: 410 cost: 0.12998 error rate: 0.039\n",
      "i: 5 j: 300 nb: 410 cost: 0.118526 error rate: 0.037\n",
      "i: 5 j: 320 nb: 410 cost: 0.115939 error rate: 0.034\n",
      "i: 5 j: 340 nb: 410 cost: 0.1127 error rate: 0.036\n",
      "i: 5 j: 360 nb: 410 cost: 0.112525 error rate: 0.034\n",
      "i: 5 j: 380 nb: 410 cost: 0.119807 error rate: 0.038\n",
      "i: 5 j: 400 nb: 410 cost: 0.113801 error rate: 0.034\n",
      "epoch: 6 n_batches: 410\n",
      "i: 6 j: 0 nb: 410 cost: 0.11039 error rate: 0.034\n",
      "i: 6 j: 20 nb: 410 cost: 0.115377 error rate: 0.037\n",
      "i: 6 j: 40 nb: 410 cost: 0.109205 error rate: 0.037\n",
      "i: 6 j: 60 nb: 410 cost: 0.105069 error rate: 0.035\n",
      "i: 6 j: 80 nb: 410 cost: 0.0993019 error rate: 0.034\n",
      "i: 6 j: 100 nb: 410 cost: 0.107715 error rate: 0.033\n",
      "i: 6 j: 120 nb: 410 cost: 0.118981 error rate: 0.036\n",
      "i: 6 j: 140 nb: 410 cost: 0.118933 error rate: 0.04\n",
      "i: 6 j: 160 nb: 410 cost: 0.118639 error rate: 0.039\n",
      "i: 6 j: 180 nb: 410 cost: 0.118651 error rate: 0.032\n",
      "i: 6 j: 200 nb: 410 cost: 0.119043 error rate: 0.031\n",
      "i: 6 j: 220 nb: 410 cost: 0.119618 error rate: 0.036\n",
      "i: 6 j: 240 nb: 410 cost: 0.122206 error rate: 0.037\n",
      "i: 6 j: 260 nb: 410 cost: 0.115674 error rate: 0.036\n",
      "i: 6 j: 280 nb: 410 cost: 0.116745 error rate: 0.039\n",
      "i: 6 j: 300 nb: 410 cost: 0.125435 error rate: 0.038\n",
      "i: 6 j: 320 nb: 410 cost: 0.115196 error rate: 0.031\n",
      "i: 6 j: 340 nb: 410 cost: 0.119372 error rate: 0.034\n",
      "i: 6 j: 360 nb: 410 cost: 0.115703 error rate: 0.039\n",
      "i: 6 j: 380 nb: 410 cost: 0.123862 error rate: 0.042\n",
      "i: 6 j: 400 nb: 410 cost: 0.116506 error rate: 0.04\n",
      "epoch: 7 n_batches: 410\n",
      "i: 7 j: 0 nb: 410 cost: 0.113816 error rate: 0.036\n",
      "i: 7 j: 20 nb: 410 cost: 0.115006 error rate: 0.036\n",
      "i: 7 j: 40 nb: 410 cost: 0.108614 error rate: 0.037\n",
      "i: 7 j: 60 nb: 410 cost: 0.1097 error rate: 0.034\n",
      "i: 7 j: 80 nb: 410 cost: 0.114784 error rate: 0.038\n",
      "i: 7 j: 100 nb: 410 cost: 0.116029 error rate: 0.035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 7 j: 120 nb: 410 cost: 0.119149 error rate: 0.035\n",
      "i: 7 j: 140 nb: 410 cost: 0.115666 error rate: 0.037\n",
      "i: 7 j: 160 nb: 410 cost: 0.119708 error rate: 0.041\n",
      "i: 7 j: 180 nb: 410 cost: 0.109258 error rate: 0.038\n",
      "i: 7 j: 200 nb: 410 cost: 0.120156 error rate: 0.041\n",
      "i: 7 j: 220 nb: 410 cost: 0.11572 error rate: 0.038\n",
      "i: 7 j: 240 nb: 410 cost: 0.111078 error rate: 0.038\n",
      "i: 7 j: 260 nb: 410 cost: 0.112086 error rate: 0.037\n",
      "i: 7 j: 280 nb: 410 cost: 0.113096 error rate: 0.036\n",
      "i: 7 j: 300 nb: 410 cost: 0.122247 error rate: 0.042\n",
      "i: 7 j: 320 nb: 410 cost: 0.120139 error rate: 0.04\n",
      "i: 7 j: 340 nb: 410 cost: 0.127476 error rate: 0.039\n",
      "i: 7 j: 360 nb: 410 cost: 0.122945 error rate: 0.041\n",
      "i: 7 j: 380 nb: 410 cost: 0.115428 error rate: 0.032\n",
      "i: 7 j: 400 nb: 410 cost: 0.130013 error rate: 0.036\n",
      "epoch: 8 n_batches: 410\n",
      "i: 8 j: 0 nb: 410 cost: 0.132988 error rate: 0.034\n",
      "i: 8 j: 20 nb: 410 cost: 0.116538 error rate: 0.032\n",
      "i: 8 j: 40 nb: 410 cost: 0.113489 error rate: 0.035\n",
      "i: 8 j: 60 nb: 410 cost: 0.11385 error rate: 0.032\n",
      "i: 8 j: 80 nb: 410 cost: 0.120625 error rate: 0.033\n",
      "i: 8 j: 100 nb: 410 cost: 0.119905 error rate: 0.038\n",
      "i: 8 j: 120 nb: 410 cost: 0.118094 error rate: 0.035\n",
      "i: 8 j: 140 nb: 410 cost: 0.117441 error rate: 0.031\n",
      "i: 8 j: 160 nb: 410 cost: 0.114514 error rate: 0.034\n",
      "i: 8 j: 180 nb: 410 cost: 0.118291 error rate: 0.034\n",
      "i: 8 j: 200 nb: 410 cost: 0.118292 error rate: 0.034\n",
      "i: 8 j: 220 nb: 410 cost: 0.115699 error rate: 0.039\n",
      "i: 8 j: 240 nb: 410 cost: 0.123434 error rate: 0.033\n",
      "i: 8 j: 260 nb: 410 cost: 0.118367 error rate: 0.036\n",
      "i: 8 j: 280 nb: 410 cost: 0.119241 error rate: 0.033\n",
      "i: 8 j: 300 nb: 410 cost: 0.121097 error rate: 0.037\n",
      "i: 8 j: 320 nb: 410 cost: 0.119399 error rate: 0.038\n",
      "i: 8 j: 340 nb: 410 cost: 0.112937 error rate: 0.036\n",
      "i: 8 j: 360 nb: 410 cost: 0.112968 error rate: 0.033\n",
      "i: 8 j: 380 nb: 410 cost: 0.116119 error rate: 0.035\n",
      "i: 8 j: 400 nb: 410 cost: 0.111795 error rate: 0.032\n",
      "epoch: 9 n_batches: 410\n",
      "i: 9 j: 0 nb: 410 cost: 0.108563 error rate: 0.032\n",
      "i: 9 j: 20 nb: 410 cost: 0.109819 error rate: 0.037\n",
      "i: 9 j: 40 nb: 410 cost: 0.118303 error rate: 0.037\n",
      "i: 9 j: 60 nb: 410 cost: 0.107935 error rate: 0.031\n",
      "i: 9 j: 80 nb: 410 cost: 0.107264 error rate: 0.031\n",
      "i: 9 j: 100 nb: 410 cost: 0.100685 error rate: 0.032\n",
      "i: 9 j: 120 nb: 410 cost: 0.105898 error rate: 0.036\n",
      "i: 9 j: 140 nb: 410 cost: 0.111622 error rate: 0.034\n",
      "i: 9 j: 160 nb: 410 cost: 0.109556 error rate: 0.036\n",
      "i: 9 j: 180 nb: 410 cost: 0.109904 error rate: 0.034\n",
      "i: 9 j: 200 nb: 410 cost: 0.111509 error rate: 0.037\n",
      "i: 9 j: 220 nb: 410 cost: 0.119284 error rate: 0.035\n",
      "i: 9 j: 240 nb: 410 cost: 0.124062 error rate: 0.038\n",
      "i: 9 j: 260 nb: 410 cost: 0.119103 error rate: 0.035\n",
      "i: 9 j: 280 nb: 410 cost: 0.113755 error rate: 0.034\n",
      "i: 9 j: 300 nb: 410 cost: 0.118323 error rate: 0.039\n",
      "i: 9 j: 320 nb: 410 cost: 0.112398 error rate: 0.036\n",
      "i: 9 j: 340 nb: 410 cost: 0.112692 error rate: 0.032\n",
      "i: 9 j: 360 nb: 410 cost: 0.10812 error rate: 0.034\n",
      "i: 9 j: 380 nb: 410 cost: 0.0988902 error rate: 0.03\n",
      "i: 9 j: 400 nb: 410 cost: 0.101968 error rate: 0.03\n",
      "epoch: 10 n_batches: 410\n",
      "i: 10 j: 0 nb: 410 cost: 0.107972 error rate: 0.031\n",
      "i: 10 j: 20 nb: 410 cost: 0.106512 error rate: 0.033\n",
      "i: 10 j: 40 nb: 410 cost: 0.112862 error rate: 0.035\n",
      "i: 10 j: 60 nb: 410 cost: 0.106118 error rate: 0.03\n",
      "i: 10 j: 80 nb: 410 cost: 0.0975892 error rate: 0.029\n",
      "i: 10 j: 100 nb: 410 cost: 0.0992207 error rate: 0.032\n",
      "i: 10 j: 120 nb: 410 cost: 0.106118 error rate: 0.031\n",
      "i: 10 j: 140 nb: 410 cost: 0.110578 error rate: 0.034\n",
      "i: 10 j: 160 nb: 410 cost: 0.111581 error rate: 0.032\n",
      "i: 10 j: 180 nb: 410 cost: 0.109259 error rate: 0.029\n",
      "i: 10 j: 200 nb: 410 cost: 0.104251 error rate: 0.033\n",
      "i: 10 j: 220 nb: 410 cost: 0.10001 error rate: 0.035\n",
      "i: 10 j: 240 nb: 410 cost: 0.102578 error rate: 0.034\n",
      "i: 10 j: 260 nb: 410 cost: 0.109023 error rate: 0.042\n",
      "i: 10 j: 280 nb: 410 cost: 0.120026 error rate: 0.04\n",
      "i: 10 j: 300 nb: 410 cost: 0.12711 error rate: 0.035\n",
      "i: 10 j: 320 nb: 410 cost: 0.105898 error rate: 0.031\n",
      "i: 10 j: 340 nb: 410 cost: 0.105565 error rate: 0.036\n",
      "i: 10 j: 360 nb: 410 cost: 0.115844 error rate: 0.035\n",
      "i: 10 j: 380 nb: 410 cost: 0.115036 error rate: 0.037\n",
      "i: 10 j: 400 nb: 410 cost: 0.114527 error rate: 0.039\n",
      "epoch: 11 n_batches: 410\n",
      "i: 11 j: 0 nb: 410 cost: 0.112194 error rate: 0.034\n",
      "i: 11 j: 20 nb: 410 cost: 0.112238 error rate: 0.035\n",
      "i: 11 j: 40 nb: 410 cost: 0.108625 error rate: 0.033\n",
      "i: 11 j: 60 nb: 410 cost: 0.112236 error rate: 0.032\n",
      "i: 11 j: 80 nb: 410 cost: 0.12392 error rate: 0.034\n",
      "i: 11 j: 100 nb: 410 cost: 0.113717 error rate: 0.032\n",
      "i: 11 j: 120 nb: 410 cost: 0.112306 error rate: 0.036\n",
      "i: 11 j: 140 nb: 410 cost: 0.11651 error rate: 0.034\n",
      "i: 11 j: 160 nb: 410 cost: 0.110308 error rate: 0.034\n",
      "i: 11 j: 180 nb: 410 cost: 0.109306 error rate: 0.032\n",
      "i: 11 j: 200 nb: 410 cost: 0.0999249 error rate: 0.033\n",
      "i: 11 j: 220 nb: 410 cost: 0.0990216 error rate: 0.031\n",
      "i: 11 j: 240 nb: 410 cost: 0.110729 error rate: 0.033\n",
      "i: 11 j: 260 nb: 410 cost: 0.102349 error rate: 0.033\n",
      "i: 11 j: 280 nb: 410 cost: 0.107995 error rate: 0.032\n",
      "i: 11 j: 300 nb: 410 cost: 0.105022 error rate: 0.03\n",
      "i: 11 j: 320 nb: 410 cost: 0.110267 error rate: 0.035\n",
      "i: 11 j: 340 nb: 410 cost: 0.115147 error rate: 0.033\n",
      "i: 11 j: 360 nb: 410 cost: 0.119474 error rate: 0.034\n",
      "i: 11 j: 380 nb: 410 cost: 0.112345 error rate: 0.032\n",
      "i: 11 j: 400 nb: 410 cost: 0.115163 error rate: 0.037\n",
      "epoch: 12 n_batches: 410\n",
      "i: 12 j: 0 nb: 410 cost: 0.114273 error rate: 0.031\n",
      "i: 12 j: 20 nb: 410 cost: 0.116353 error rate: 0.03\n",
      "i: 12 j: 40 nb: 410 cost: 0.110297 error rate: 0.033\n",
      "i: 12 j: 60 nb: 410 cost: 0.110579 error rate: 0.031\n",
      "i: 12 j: 80 nb: 410 cost: 0.107246 error rate: 0.031\n",
      "i: 12 j: 100 nb: 410 cost: 0.109336 error rate: 0.033\n",
      "i: 12 j: 120 nb: 410 cost: 0.109303 error rate: 0.035\n",
      "i: 12 j: 140 nb: 410 cost: 0.113624 error rate: 0.038\n",
      "i: 12 j: 160 nb: 410 cost: 0.109816 error rate: 0.029\n",
      "i: 12 j: 180 nb: 410 cost: 0.107404 error rate: 0.029\n",
      "i: 12 j: 200 nb: 410 cost: 0.111193 error rate: 0.029\n",
      "i: 12 j: 220 nb: 410 cost: 0.116129 error rate: 0.033\n",
      "i: 12 j: 240 nb: 410 cost: 0.121621 error rate: 0.033\n",
      "i: 12 j: 260 nb: 410 cost: 0.119166 error rate: 0.034\n",
      "i: 12 j: 280 nb: 410 cost: 0.114395 error rate: 0.029\n",
      "i: 12 j: 300 nb: 410 cost: 0.115096 error rate: 0.036\n",
      "i: 12 j: 320 nb: 410 cost: 0.109733 error rate: 0.033\n",
      "i: 12 j: 340 nb: 410 cost: 0.109347 error rate: 0.033\n",
      "i: 12 j: 360 nb: 410 cost: 0.108912 error rate: 0.032\n",
      "i: 12 j: 380 nb: 410 cost: 0.109791 error rate: 0.036\n",
      "i: 12 j: 400 nb: 410 cost: 0.112689 error rate: 0.035\n",
      "epoch: 13 n_batches: 410\n",
      "i: 13 j: 0 nb: 410 cost: 0.105466 error rate: 0.033\n",
      "i: 13 j: 20 nb: 410 cost: 0.103387 error rate: 0.036\n",
      "i: 13 j: 40 nb: 410 cost: 0.102736 error rate: 0.035\n",
      "i: 13 j: 60 nb: 410 cost: 0.109141 error rate: 0.033\n",
      "i: 13 j: 80 nb: 410 cost: 0.110537 error rate: 0.038\n",
      "i: 13 j: 100 nb: 410 cost: 0.111795 error rate: 0.031\n",
      "i: 13 j: 120 nb: 410 cost: 0.115814 error rate: 0.029\n",
      "i: 13 j: 140 nb: 410 cost: 0.111277 error rate: 0.026\n",
      "i: 13 j: 160 nb: 410 cost: 0.119836 error rate: 0.028\n",
      "i: 13 j: 180 nb: 410 cost: 0.12364 error rate: 0.033\n",
      "i: 13 j: 200 nb: 410 cost: 0.127362 error rate: 0.032\n",
      "i: 13 j: 220 nb: 410 cost: 0.118014 error rate: 0.031\n",
      "i: 13 j: 240 nb: 410 cost: 0.110896 error rate: 0.033\n",
      "i: 13 j: 260 nb: 410 cost: 0.106497 error rate: 0.031\n",
      "i: 13 j: 280 nb: 410 cost: 0.110667 error rate: 0.034\n",
      "i: 13 j: 300 nb: 410 cost: 0.117094 error rate: 0.032\n",
      "i: 13 j: 320 nb: 410 cost: 0.111933 error rate: 0.028\n",
      "i: 13 j: 340 nb: 410 cost: 0.104608 error rate: 0.032\n",
      "i: 13 j: 360 nb: 410 cost: 0.108154 error rate: 0.029\n",
      "i: 13 j: 380 nb: 410 cost: 0.112253 error rate: 0.035\n",
      "i: 13 j: 400 nb: 410 cost: 0.113766 error rate: 0.032\n",
      "epoch: 14 n_batches: 410\n",
      "i: 14 j: 0 nb: 410 cost: 0.115682 error rate: 0.035\n",
      "i: 14 j: 20 nb: 410 cost: 0.116397 error rate: 0.036\n",
      "i: 14 j: 40 nb: 410 cost: 0.121207 error rate: 0.035\n",
      "i: 14 j: 60 nb: 410 cost: 0.128517 error rate: 0.041\n",
      "i: 14 j: 80 nb: 410 cost: 0.12435 error rate: 0.033\n",
      "i: 14 j: 100 nb: 410 cost: 0.117932 error rate: 0.034\n",
      "i: 14 j: 120 nb: 410 cost: 0.118488 error rate: 0.033\n",
      "i: 14 j: 140 nb: 410 cost: 0.123736 error rate: 0.035\n",
      "i: 14 j: 160 nb: 410 cost: 0.122575 error rate: 0.035\n",
      "i: 14 j: 180 nb: 410 cost: 0.132861 error rate: 0.036\n",
      "i: 14 j: 200 nb: 410 cost: 0.126533 error rate: 0.035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 14 j: 220 nb: 410 cost: 0.121841 error rate: 0.033\n",
      "i: 14 j: 240 nb: 410 cost: 0.113488 error rate: 0.03\n",
      "i: 14 j: 260 nb: 410 cost: 0.119508 error rate: 0.028\n",
      "i: 14 j: 280 nb: 410 cost: 0.10811 error rate: 0.029\n",
      "i: 14 j: 300 nb: 410 cost: 0.111424 error rate: 0.03\n",
      "i: 14 j: 320 nb: 410 cost: 0.113026 error rate: 0.026\n",
      "i: 14 j: 340 nb: 410 cost: 0.121129 error rate: 0.029\n",
      "i: 14 j: 360 nb: 410 cost: 0.117037 error rate: 0.033\n",
      "i: 14 j: 380 nb: 410 cost: 0.108878 error rate: 0.035\n",
      "i: 14 j: 400 nb: 410 cost: 0.109882 error rate: 0.031\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUoAU7iIiBej/A1mqQTB3fhDrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc05003ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# step 1: get the data and define all the usual variables\n",
    "X, Y = get_normalized_data()\n",
    "\n",
    "ann = ANN([500, 300], [0.8, 0.5, 0.5])\n",
    "ann.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
