{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denis\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from util import getData, getBinaryData, y2indicator, error_rate, init_weight_and_bias\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, M1, M2, an_id):\n",
    "        self.id = an_id\n",
    "        self.M1 = M1\n",
    "        self.M2 = M2\n",
    "        W, b = init_weight_and_bias(M1, M2)\n",
    "        self.W = tf.Variable(W.astype(np.float32))\n",
    "        self.b = tf.Variable(b.astype(np.float32))\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return tf.nn.relu(tf.matmul(X, self.W) + self.b)\n",
    "    \n",
    "class ANN(object):\n",
    "    def __init__(self, hidden_layer_sizes):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        \n",
    "    def fit(self, X, Y, learning_rate=10e-7, mu=0.99, decay=0.999, reg=10e-3,\n",
    "           epochs=400, batch_sz=100, show_fig=False):\n",
    "        K = len(set(Y))\n",
    "        \n",
    "        #make a validation set\n",
    "        X, Y = shuffle(X, Y)\n",
    "        X = X.astype(np.float32)\n",
    "        Y = y2indicator(Y).astype(np.float32)\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "        Yvalid_flat = np.argmax(Yvalid, axis=1)\n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "        \n",
    "        #initialize hidden layers\n",
    "        N, D = X.shape\n",
    "        self.hidden_layers = []\n",
    "        M1 = D\n",
    "        count = 0\n",
    "        for M2 in self.hidden_layer_sizes:\n",
    "            h = HiddenLayer(M1, M2, count)\n",
    "            self.hidden_layers.append(h)\n",
    "            M1 = M2\n",
    "            count += 1\n",
    "        W, b = init_weight_and_bias(M1, K)\n",
    "        self.W = tf.Variable(W.astype(np.float32))\n",
    "        self.b = tf.Variable(b.astype(np.float32))\n",
    "        \n",
    "        # collect params\n",
    "        self.params = [self.W, self.b]\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += h.params\n",
    "            \n",
    "        tfX = tf.placeholder(tf.float32, shape=(None, D), name='X')\n",
    "        tfT = tf.placeholder(tf.float32, shape=(None, K), name='T')\n",
    "        act = self.forward(tfX)\n",
    "        \n",
    "        rcost = reg*sum([tf.nn.l2_loss(p) for p in self.params]) #regularization cost\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=act, labels=tfT)) + rcost\n",
    "        prediction = self.predict(tfX)\n",
    "        train_op = tf.train.RMSPropOptimizer(learning_rate, decay=decay, momentum=mu).minimize(cost)\n",
    "        \n",
    "        n_batches = N // batch_sz\n",
    "        costs = []\n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            for i in range(epochs):\n",
    "                X, Y = shuffle(X, Y)\n",
    "                for j in range(n_batches):\n",
    "                    Xbatch = X[j*batch_sz:(j*batch_sz + batch_sz)]\n",
    "                    Ybatch = Y[j*batch_sz:(j*batch_sz + batch_sz)]\n",
    "                    \n",
    "                    session.run(train_op, feed_dict={tfX: Xbatch, tfT: Ybatch})\n",
    "                    \n",
    "                    if j % 20 == 0:\n",
    "                        c = session.run(cost, feed_dict={tfX: Xvalid, tfT: Yvalid})\n",
    "                        costs.append(c)\n",
    "                        \n",
    "                        p = session.run(prediction, feed_dict={tfX: Xvalid, tfT: Yvalid})\n",
    "                        e = error_rate(Yvalid_flat, p)\n",
    "                print(\"i:\", i, \"j:\", j, \"nb:\", n_batches, \"cost:\", c, \"error_rate:\", e)\n",
    "                        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "            \n",
    "    def forward(self, X):\n",
    "        Z = X\n",
    "        for h in self.hidden_layers:\n",
    "            Z = h.forward(Z)\n",
    "        return tf.matmul(Z, self.W) + self.b\n",
    "    \n",
    "    def predict(self, X):\n",
    "        act = self.forward(X)\n",
    "        return tf.argmax(act, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>85 84 90 121 101 102 133 153 153 169 177 189 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>255 254 255 254 254 179 122 107 95 124 149 150...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>30 24 21 23 25 25 49 67 84 103 120 125 130 139...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>39 75 78 58 58 45 49 48 103 156 81 45 41 38 49...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>219 213 206 202 209 217 216 215 219 218 223 23...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>148 144 130 129 119 122 129 131 139 153 140 12...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>4 2 13 41 56 62 67 87 95 62 65 70 80 107 127 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>107 107 109 109 109 109 110 101 123 140 144 14...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>14 14 18 28 27 22 21 30 42 61 77 86 88 95 100 ...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>255 255 255 255 255 255 255 255 255 255 255 25...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>134 124 167 180 197 194 203 210 204 203 209 20...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>219 192 179 148 208 254 192 98 121 103 145 185...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 7 12 23 45 38 ...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>174 51 37 37 38 41 22 25 22 24 35 51 70 83 98 ...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>123 125 124 142 209 226 234 236 231 232 235 22...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>8 9 14 21 26 32 37 46 52 62 72 70 71 73 76 83 ...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>252 250 246 229 182 140 98 72 53 44 67 95 95 8...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>224 227 219 217 215 210 187 177 189 200 206 21...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5</td>\n",
       "      <td>162 200 187 180 197 198 196 192 176 152 136 11...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>236 230 225 226 228 209 199 193 196 211 199 19...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>210 210 210 210 211 207 147 103 68 60 47 70 12...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>50 44 74 141 187 187 169 113 80 128 181 172 76...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35857</th>\n",
       "      <td>5</td>\n",
       "      <td>253 255 229 150 89 61 54 60 55 49 61 50 56 45 ...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35858</th>\n",
       "      <td>4</td>\n",
       "      <td>11 11 11 13 20 27 38 41 38 34 20 13 10 39 85 1...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35859</th>\n",
       "      <td>4</td>\n",
       "      <td>11 13 16 27 24 26 89 161 190 197 201 206 210 2...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35860</th>\n",
       "      <td>3</td>\n",
       "      <td>27 42 62 91 112 118 122 123 119 124 129 131 13...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35861</th>\n",
       "      <td>6</td>\n",
       "      <td>233 232 208 188 194 179 177 167 157 180 185 19...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35862</th>\n",
       "      <td>2</td>\n",
       "      <td>73 54 63 76 82 71 67 69 73 72 92 98 117 119 14...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35863</th>\n",
       "      <td>5</td>\n",
       "      <td>196 196 197 197 198 198 198 196 176 148 122 10...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35864</th>\n",
       "      <td>4</td>\n",
       "      <td>68 59 65 78 118 131 137 141 142 135 135 137 13...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35865</th>\n",
       "      <td>3</td>\n",
       "      <td>102 109 109 106 104 107 112 109 116 119 117 12...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35866</th>\n",
       "      <td>6</td>\n",
       "      <td>87 82 59 61 72 102 143 130 90 95 143 173 146 1...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35867</th>\n",
       "      <td>3</td>\n",
       "      <td>198 198 197 196 196 197 196 196 196 195 196 18...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35868</th>\n",
       "      <td>2</td>\n",
       "      <td>204 209 215 218 214 214 214 217 205 175 170 16...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35869</th>\n",
       "      <td>3</td>\n",
       "      <td>217 220 222 223 223 224 225 223 223 225 223 22...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35870</th>\n",
       "      <td>2</td>\n",
       "      <td>6 8 4 5 30 48 61 70 76 79 98 117 130 137 143 1...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35871</th>\n",
       "      <td>6</td>\n",
       "      <td>112 102 98 89 98 133 164 185 180 179 185 169 1...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35872</th>\n",
       "      <td>5</td>\n",
       "      <td>131 159 90 59 10 0 1 1 1 0 1 1 0 0 2 2 5 7 9 1...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35873</th>\n",
       "      <td>4</td>\n",
       "      <td>54 57 77 122 121 76 73 80 58 22 26 27 35 41 66...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35874</th>\n",
       "      <td>5</td>\n",
       "      <td>43 43 51 73 94 97 102 95 99 107 126 144 154 17...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35875</th>\n",
       "      <td>5</td>\n",
       "      <td>248 251 239 144 102 95 82 77 91 138 153 145 14...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35876</th>\n",
       "      <td>6</td>\n",
       "      <td>29 29 27 31 49 56 29 19 22 20 34 43 55 71 85 9...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35877</th>\n",
       "      <td>6</td>\n",
       "      <td>139 143 145 154 159 168 176 181 190 191 195 19...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35878</th>\n",
       "      <td>3</td>\n",
       "      <td>0 39 81 80 104 97 51 64 68 46 41 67 53 68 70 5...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35879</th>\n",
       "      <td>2</td>\n",
       "      <td>0 0 6 16 19 31 47 18 26 19 17 8 15 3 4 2 14 20...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35880</th>\n",
       "      <td>2</td>\n",
       "      <td>164 172 175 171 172 173 178 181 188 192 197 20...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35881</th>\n",
       "      <td>0</td>\n",
       "      <td>181 177 176 156 178 144 136 132 122 107 131 16...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35882</th>\n",
       "      <td>6</td>\n",
       "      <td>50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35883</th>\n",
       "      <td>3</td>\n",
       "      <td>178 174 172 173 181 188 191 194 196 199 200 20...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35884</th>\n",
       "      <td>0</td>\n",
       "      <td>17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35885</th>\n",
       "      <td>3</td>\n",
       "      <td>30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35886</th>\n",
       "      <td>2</td>\n",
       "      <td>19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35887 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       emotion                                             pixels        Usage\n",
       "0            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...     Training\n",
       "1            0  151 150 147 155 148 133 111 140 170 174 182 15...     Training\n",
       "2            2  231 212 156 164 174 138 161 173 182 200 106 38...     Training\n",
       "3            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...     Training\n",
       "4            6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...     Training\n",
       "5            2  55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...     Training\n",
       "6            4  20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...     Training\n",
       "7            3  77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...     Training\n",
       "8            3  85 84 90 121 101 102 133 153 153 169 177 189 1...     Training\n",
       "9            2  255 254 255 254 254 179 122 107 95 124 149 150...     Training\n",
       "10           0  30 24 21 23 25 25 49 67 84 103 120 125 130 139...     Training\n",
       "11           6  39 75 78 58 58 45 49 48 103 156 81 45 41 38 49...     Training\n",
       "12           6  219 213 206 202 209 217 216 215 219 218 223 23...     Training\n",
       "13           6  148 144 130 129 119 122 129 131 139 153 140 12...     Training\n",
       "14           3  4 2 13 41 56 62 67 87 95 62 65 70 80 107 127 1...     Training\n",
       "15           5  107 107 109 109 109 109 110 101 123 140 144 14...     Training\n",
       "16           3  14 14 18 28 27 22 21 30 42 61 77 86 88 95 100 ...     Training\n",
       "17           2  255 255 255 255 255 255 255 255 255 255 255 25...     Training\n",
       "18           6  134 124 167 180 197 194 203 210 204 203 209 20...     Training\n",
       "19           4  219 192 179 148 208 254 192 98 121 103 145 185...     Training\n",
       "20           4  1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 7 12 23 45 38 ...     Training\n",
       "21           2  174 51 37 37 38 41 22 25 22 24 35 51 70 83 98 ...     Training\n",
       "22           0  123 125 124 142 209 226 234 236 231 232 235 22...     Training\n",
       "23           0  8 9 14 21 26 32 37 46 52 62 72 70 71 73 76 83 ...     Training\n",
       "24           3  252 250 246 229 182 140 98 72 53 44 67 95 95 8...     Training\n",
       "25           3  224 227 219 217 215 210 187 177 189 200 206 21...     Training\n",
       "26           5  162 200 187 180 197 198 196 192 176 152 136 11...     Training\n",
       "27           0  236 230 225 226 228 209 199 193 196 211 199 19...     Training\n",
       "28           3  210 210 210 210 211 207 147 103 68 60 47 70 12...     Training\n",
       "29           5  50 44 74 141 187 187 169 113 80 128 181 172 76...     Training\n",
       "...        ...                                                ...          ...\n",
       "35857        5  253 255 229 150 89 61 54 60 55 49 61 50 56 45 ...  PrivateTest\n",
       "35858        4  11 11 11 13 20 27 38 41 38 34 20 13 10 39 85 1...  PrivateTest\n",
       "35859        4  11 13 16 27 24 26 89 161 190 197 201 206 210 2...  PrivateTest\n",
       "35860        3  27 42 62 91 112 118 122 123 119 124 129 131 13...  PrivateTest\n",
       "35861        6  233 232 208 188 194 179 177 167 157 180 185 19...  PrivateTest\n",
       "35862        2  73 54 63 76 82 71 67 69 73 72 92 98 117 119 14...  PrivateTest\n",
       "35863        5  196 196 197 197 198 198 198 196 176 148 122 10...  PrivateTest\n",
       "35864        4  68 59 65 78 118 131 137 141 142 135 135 137 13...  PrivateTest\n",
       "35865        3  102 109 109 106 104 107 112 109 116 119 117 12...  PrivateTest\n",
       "35866        6  87 82 59 61 72 102 143 130 90 95 143 173 146 1...  PrivateTest\n",
       "35867        3  198 198 197 196 196 197 196 196 196 195 196 18...  PrivateTest\n",
       "35868        2  204 209 215 218 214 214 214 217 205 175 170 16...  PrivateTest\n",
       "35869        3  217 220 222 223 223 224 225 223 223 225 223 22...  PrivateTest\n",
       "35870        2  6 8 4 5 30 48 61 70 76 79 98 117 130 137 143 1...  PrivateTest\n",
       "35871        6  112 102 98 89 98 133 164 185 180 179 185 169 1...  PrivateTest\n",
       "35872        5  131 159 90 59 10 0 1 1 1 0 1 1 0 0 2 2 5 7 9 1...  PrivateTest\n",
       "35873        4  54 57 77 122 121 76 73 80 58 22 26 27 35 41 66...  PrivateTest\n",
       "35874        5  43 43 51 73 94 97 102 95 99 107 126 144 154 17...  PrivateTest\n",
       "35875        5  248 251 239 144 102 95 82 77 91 138 153 145 14...  PrivateTest\n",
       "35876        6  29 29 27 31 49 56 29 19 22 20 34 43 55 71 85 9...  PrivateTest\n",
       "35877        6  139 143 145 154 159 168 176 181 190 191 195 19...  PrivateTest\n",
       "35878        3  0 39 81 80 104 97 51 64 68 46 41 67 53 68 70 5...  PrivateTest\n",
       "35879        2  0 0 6 16 19 31 47 18 26 19 17 8 15 3 4 2 14 20...  PrivateTest\n",
       "35880        2  164 172 175 171 172 173 178 181 188 192 197 20...  PrivateTest\n",
       "35881        0  181 177 176 156 178 144 136 132 122 107 131 16...  PrivateTest\n",
       "35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n",
       "35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n",
       "35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n",
       "35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n",
       "35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest\n",
       "\n",
       "[35887 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('c:/Users/Denis/Desktop/fer2013/fer2013.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in df.iloc[:, 1]:\n",
    "    X.append(i.split(' '))\n",
    "X = np.array(X, dtype=np.float32)\n",
    "Y = df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 j: 347 nb: 348 cost: 12.864573 error_rate: 0.837\n",
      "i: 1 j: 347 nb: 348 cost: 12.44277 error_rate: 0.806\n",
      "i: 2 j: 347 nb: 348 cost: 12.3983 error_rate: 0.791\n",
      "i: 3 j: 347 nb: 348 cost: 12.317645 error_rate: 0.821\n",
      "i: 4 j: 347 nb: 348 cost: 12.2943535 error_rate: 0.83\n",
      "i: 5 j: 347 nb: 348 cost: 12.293301 error_rate: 0.817\n",
      "i: 6 j: 347 nb: 348 cost: 12.238659 error_rate: 0.806\n",
      "i: 7 j: 347 nb: 348 cost: 12.213974 error_rate: 0.809\n",
      "i: 8 j: 347 nb: 348 cost: 12.190495 error_rate: 0.815\n",
      "i: 9 j: 347 nb: 348 cost: 12.101008 error_rate: 0.802\n",
      "i: 10 j: 347 nb: 348 cost: 12.058794 error_rate: 0.794\n",
      "i: 11 j: 347 nb: 348 cost: 12.064694 error_rate: 0.804\n",
      "i: 12 j: 347 nb: 348 cost: 11.961514 error_rate: 0.796\n",
      "i: 13 j: 347 nb: 348 cost: 11.876508 error_rate: 0.808\n",
      "i: 14 j: 347 nb: 348 cost: 11.805228 error_rate: 0.815\n",
      "i: 15 j: 347 nb: 348 cost: 11.7499695 error_rate: 0.796\n",
      "i: 16 j: 347 nb: 348 cost: 11.593809 error_rate: 0.797\n",
      "i: 17 j: 347 nb: 348 cost: 11.4523115 error_rate: 0.81\n",
      "i: 18 j: 347 nb: 348 cost: 11.302504 error_rate: 0.789\n",
      "i: 19 j: 347 nb: 348 cost: 11.123086 error_rate: 0.796\n",
      "i: 20 j: 347 nb: 348 cost: 10.926493 error_rate: 0.803\n",
      "i: 21 j: 347 nb: 348 cost: 10.65271 error_rate: 0.796\n",
      "i: 22 j: 347 nb: 348 cost: 10.382879 error_rate: 0.819\n",
      "i: 23 j: 347 nb: 348 cost: 10.066261 error_rate: 0.786\n",
      "i: 24 j: 347 nb: 348 cost: 9.667713 error_rate: 0.793\n",
      "i: 25 j: 347 nb: 348 cost: 9.266816 error_rate: 0.789\n",
      "i: 26 j: 347 nb: 348 cost: 8.817174 error_rate: 0.776\n",
      "i: 27 j: 347 nb: 348 cost: 8.316249 error_rate: 0.809\n",
      "i: 28 j: 347 nb: 348 cost: 7.7519393 error_rate: 0.797\n",
      "i: 29 j: 347 nb: 348 cost: 7.163452 error_rate: 0.812\n",
      "i: 30 j: 347 nb: 348 cost: 6.539094 error_rate: 0.777\n",
      "i: 31 j: 347 nb: 348 cost: 5.9088397 error_rate: 0.811\n",
      "i: 32 j: 347 nb: 348 cost: 5.2718697 error_rate: 0.778\n",
      "i: 33 j: 347 nb: 348 cost: 4.621225 error_rate: 0.778\n",
      "i: 34 j: 347 nb: 348 cost: 4.0367513 error_rate: 0.777\n",
      "i: 35 j: 347 nb: 348 cost: 3.4950995 error_rate: 0.78\n",
      "i: 36 j: 347 nb: 348 cost: 3.0390754 error_rate: 0.78\n",
      "i: 37 j: 347 nb: 348 cost: 2.667608 error_rate: 0.78\n",
      "i: 38 j: 347 nb: 348 cost: 2.3790805 error_rate: 0.78\n",
      "i: 39 j: 347 nb: 348 cost: 2.172793 error_rate: 0.78\n",
      "i: 40 j: 347 nb: 348 cost: 2.042567 error_rate: 0.779\n",
      "i: 41 j: 347 nb: 348 cost: 1.9625505 error_rate: 0.78\n",
      "i: 42 j: 347 nb: 348 cost: 1.9227159 error_rate: 0.78\n",
      "i: 43 j: 347 nb: 348 cost: 1.8954371 error_rate: 0.78\n",
      "i: 44 j: 347 nb: 348 cost: 1.8798263 error_rate: 0.78\n",
      "i: 45 j: 347 nb: 348 cost: 1.86685 error_rate: 0.78\n",
      "i: 46 j: 347 nb: 348 cost: 1.8626146 error_rate: 0.78\n",
      "i: 47 j: 347 nb: 348 cost: 1.8557806 error_rate: 0.78\n",
      "i: 48 j: 347 nb: 348 cost: 1.8535299 error_rate: 0.78\n",
      "i: 49 j: 347 nb: 348 cost: 1.8529634 error_rate: 0.78\n",
      "i: 50 j: 347 nb: 348 cost: 1.8517902 error_rate: 0.78\n",
      "i: 51 j: 347 nb: 348 cost: 1.8490342 error_rate: 0.78\n",
      "i: 52 j: 347 nb: 348 cost: 1.8529539 error_rate: 0.78\n",
      "i: 53 j: 347 nb: 348 cost: 1.8467456 error_rate: 0.78\n",
      "i: 54 j: 347 nb: 348 cost: 1.8563097 error_rate: 0.78\n",
      "i: 55 j: 347 nb: 348 cost: 1.86901 error_rate: 0.78\n",
      "i: 56 j: 347 nb: 348 cost: 1.8561847 error_rate: 0.78\n",
      "i: 57 j: 347 nb: 348 cost: 1.8793406 error_rate: 0.78\n",
      "i: 58 j: 347 nb: 348 cost: 1.8834293 error_rate: 0.78\n",
      "i: 59 j: 347 nb: 348 cost: 1.8773915 error_rate: 0.78\n",
      "i: 60 j: 347 nb: 348 cost: 1.8729221 error_rate: 0.78\n",
      "i: 61 j: 347 nb: 348 cost: 1.8716482 error_rate: 0.78\n",
      "i: 62 j: 347 nb: 348 cost: 1.8630339 error_rate: 0.78\n",
      "i: 63 j: 347 nb: 348 cost: 1.8649582 error_rate: 0.78\n",
      "i: 64 j: 347 nb: 348 cost: 1.85944 error_rate: 0.78\n",
      "i: 65 j: 347 nb: 348 cost: 1.8541896 error_rate: 0.78\n",
      "i: 66 j: 347 nb: 348 cost: 1.8537339 error_rate: 0.78\n",
      "i: 67 j: 347 nb: 348 cost: 1.8604178 error_rate: 0.78\n",
      "i: 68 j: 347 nb: 348 cost: 1.8560644 error_rate: 0.78\n",
      "i: 69 j: 347 nb: 348 cost: 1.8519676 error_rate: 0.78\n",
      "i: 70 j: 347 nb: 348 cost: 1.8507832 error_rate: 0.78\n",
      "i: 71 j: 347 nb: 348 cost: 1.8487273 error_rate: 0.78\n",
      "i: 72 j: 347 nb: 348 cost: 1.8421689 error_rate: 0.78\n",
      "i: 73 j: 347 nb: 348 cost: 1.8410201 error_rate: 0.78\n",
      "i: 74 j: 347 nb: 348 cost: 1.8396776 error_rate: 0.78\n",
      "i: 75 j: 347 nb: 348 cost: 1.8414752 error_rate: 0.78\n",
      "i: 76 j: 347 nb: 348 cost: 1.8457544 error_rate: 0.78\n",
      "i: 77 j: 347 nb: 348 cost: 1.845835 error_rate: 0.78\n",
      "i: 78 j: 347 nb: 348 cost: 1.8546982 error_rate: 0.78\n",
      "i: 79 j: 347 nb: 348 cost: 1.8552635 error_rate: 0.78\n",
      "i: 80 j: 347 nb: 348 cost: 1.85491 error_rate: 0.78\n",
      "i: 81 j: 347 nb: 348 cost: 1.8489802 error_rate: 0.78\n",
      "i: 82 j: 347 nb: 348 cost: 1.8371892 error_rate: 0.78\n",
      "i: 83 j: 347 nb: 348 cost: 1.8395102 error_rate: 0.78\n",
      "i: 84 j: 347 nb: 348 cost: 1.8512983 error_rate: 0.78\n",
      "i: 85 j: 347 nb: 348 cost: 1.8547645 error_rate: 0.78\n",
      "i: 86 j: 347 nb: 348 cost: 1.8390279 error_rate: 0.78\n",
      "i: 87 j: 347 nb: 348 cost: 1.8378689 error_rate: 0.78\n",
      "i: 88 j: 347 nb: 348 cost: 1.8443004 error_rate: 0.78\n",
      "i: 89 j: 347 nb: 348 cost: 1.8462037 error_rate: 0.78\n",
      "i: 90 j: 347 nb: 348 cost: 1.8378794 error_rate: 0.78\n",
      "i: 91 j: 347 nb: 348 cost: 1.8347422 error_rate: 0.78\n",
      "i: 92 j: 347 nb: 348 cost: 1.8339769 error_rate: 0.78\n",
      "i: 93 j: 347 nb: 348 cost: 1.8406922 error_rate: 0.78\n",
      "i: 94 j: 347 nb: 348 cost: 1.844976 error_rate: 0.78\n",
      "i: 95 j: 347 nb: 348 cost: 1.8379241 error_rate: 0.78\n",
      "i: 96 j: 347 nb: 348 cost: 1.8357897 error_rate: 0.78\n",
      "i: 97 j: 347 nb: 348 cost: 1.8418174 error_rate: 0.78\n",
      "i: 98 j: 347 nb: 348 cost: 1.8496668 error_rate: 0.78\n",
      "i: 99 j: 347 nb: 348 cost: 1.8419869 error_rate: 0.78\n",
      "i: 100 j: 347 nb: 348 cost: 1.8371854 error_rate: 0.78\n",
      "i: 101 j: 347 nb: 348 cost: 1.8398539 error_rate: 0.78\n",
      "i: 102 j: 347 nb: 348 cost: 1.8369988 error_rate: 0.78\n",
      "i: 103 j: 347 nb: 348 cost: 1.8368332 error_rate: 0.78\n",
      "i: 104 j: 347 nb: 348 cost: 1.8424138 error_rate: 0.78\n",
      "i: 105 j: 347 nb: 348 cost: 1.8530418 error_rate: 0.78\n",
      "i: 106 j: 347 nb: 348 cost: 1.8397425 error_rate: 0.78\n",
      "i: 107 j: 347 nb: 348 cost: 1.8424692 error_rate: 0.78\n",
      "i: 108 j: 347 nb: 348 cost: 1.8440832 error_rate: 0.78\n",
      "i: 109 j: 347 nb: 348 cost: 1.8415756 error_rate: 0.78\n",
      "i: 110 j: 347 nb: 348 cost: 1.844614 error_rate: 0.78\n",
      "i: 111 j: 347 nb: 348 cost: 1.8431262 error_rate: 0.78\n",
      "i: 112 j: 347 nb: 348 cost: 1.8457257 error_rate: 0.78\n",
      "i: 113 j: 347 nb: 348 cost: 1.8564477 error_rate: 0.78\n",
      "i: 114 j: 347 nb: 348 cost: 1.8445258 error_rate: 0.78\n",
      "i: 115 j: 347 nb: 348 cost: 1.8465343 error_rate: 0.78\n",
      "i: 116 j: 347 nb: 348 cost: 1.8448515 error_rate: 0.78\n",
      "i: 117 j: 347 nb: 348 cost: 1.8378117 error_rate: 0.78\n",
      "i: 118 j: 347 nb: 348 cost: 1.8344535 error_rate: 0.78\n",
      "i: 119 j: 347 nb: 348 cost: 1.8331997 error_rate: 0.78\n",
      "i: 120 j: 347 nb: 348 cost: 1.8327854 error_rate: 0.78\n",
      "i: 121 j: 347 nb: 348 cost: 1.8375995 error_rate: 0.78\n",
      "i: 122 j: 347 nb: 348 cost: 1.8409026 error_rate: 0.78\n",
      "i: 123 j: 347 nb: 348 cost: 1.8493152 error_rate: 0.78\n",
      "i: 124 j: 347 nb: 348 cost: 1.8375001 error_rate: 0.78\n",
      "i: 125 j: 347 nb: 348 cost: 1.8351624 error_rate: 0.78\n",
      "i: 126 j: 347 nb: 348 cost: 1.8377134 error_rate: 0.78\n",
      "i: 127 j: 347 nb: 348 cost: 1.8340813 error_rate: 0.78\n",
      "i: 128 j: 347 nb: 348 cost: 1.8388194 error_rate: 0.78\n",
      "i: 129 j: 347 nb: 348 cost: 1.8339126 error_rate: 0.78\n",
      "i: 130 j: 347 nb: 348 cost: 1.8352295 error_rate: 0.78\n",
      "i: 131 j: 347 nb: 348 cost: 1.8343784 error_rate: 0.78\n",
      "i: 132 j: 347 nb: 348 cost: 1.8337164 error_rate: 0.78\n",
      "i: 133 j: 347 nb: 348 cost: 1.8326799 error_rate: 0.78\n",
      "i: 134 j: 347 nb: 348 cost: 1.8398771 error_rate: 0.78\n",
      "i: 135 j: 347 nb: 348 cost: 1.8416716 error_rate: 0.78\n",
      "i: 136 j: 347 nb: 348 cost: 1.8443619 error_rate: 0.78\n",
      "i: 137 j: 347 nb: 348 cost: 1.8419331 error_rate: 0.78\n",
      "i: 138 j: 347 nb: 348 cost: 1.8410788 error_rate: 0.78\n",
      "i: 139 j: 347 nb: 348 cost: 1.8391672 error_rate: 0.78\n",
      "i: 140 j: 347 nb: 348 cost: 1.8385733 error_rate: 0.78\n",
      "i: 141 j: 347 nb: 348 cost: 1.8424548 error_rate: 0.78\n",
      "i: 142 j: 347 nb: 348 cost: 1.8417344 error_rate: 0.78\n",
      "i: 143 j: 347 nb: 348 cost: 1.8390821 error_rate: 0.78\n",
      "i: 144 j: 347 nb: 348 cost: 1.8374772 error_rate: 0.78\n",
      "i: 145 j: 347 nb: 348 cost: 1.8326654 error_rate: 0.78\n",
      "i: 146 j: 347 nb: 348 cost: 1.8320785 error_rate: 0.78\n",
      "i: 147 j: 347 nb: 348 cost: 1.8339593 error_rate: 0.78\n",
      "i: 148 j: 347 nb: 348 cost: 1.8359399 error_rate: 0.78\n",
      "i: 149 j: 347 nb: 348 cost: 1.8311235 error_rate: 0.78\n",
      "i: 150 j: 347 nb: 348 cost: 1.8338447 error_rate: 0.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 151 j: 347 nb: 348 cost: 1.8341395 error_rate: 0.78\n",
      "i: 152 j: 347 nb: 348 cost: 1.8380272 error_rate: 0.78\n",
      "i: 153 j: 347 nb: 348 cost: 1.8341323 error_rate: 0.78\n",
      "i: 154 j: 347 nb: 348 cost: 1.8376483 error_rate: 0.78\n",
      "i: 155 j: 347 nb: 348 cost: 1.8359082 error_rate: 0.78\n",
      "i: 156 j: 347 nb: 348 cost: 1.8406163 error_rate: 0.78\n",
      "i: 157 j: 347 nb: 348 cost: 1.8413316 error_rate: 0.78\n",
      "i: 158 j: 347 nb: 348 cost: 1.837612 error_rate: 0.78\n",
      "i: 159 j: 347 nb: 348 cost: 1.8346695 error_rate: 0.78\n",
      "i: 160 j: 347 nb: 348 cost: 1.834041 error_rate: 0.78\n",
      "i: 161 j: 347 nb: 348 cost: 1.8365517 error_rate: 0.78\n",
      "i: 162 j: 347 nb: 348 cost: 1.8366183 error_rate: 0.78\n",
      "i: 163 j: 347 nb: 348 cost: 1.8339884 error_rate: 0.78\n",
      "i: 164 j: 347 nb: 348 cost: 1.8375964 error_rate: 0.78\n",
      "i: 165 j: 347 nb: 348 cost: 1.8337529 error_rate: 0.78\n",
      "i: 166 j: 347 nb: 348 cost: 1.832432 error_rate: 0.78\n",
      "i: 167 j: 347 nb: 348 cost: 1.8321741 error_rate: 0.78\n",
      "i: 168 j: 347 nb: 348 cost: 1.8354601 error_rate: 0.78\n",
      "i: 169 j: 347 nb: 348 cost: 1.836106 error_rate: 0.78\n",
      "i: 170 j: 347 nb: 348 cost: 1.8480899 error_rate: 0.78\n",
      "i: 171 j: 347 nb: 348 cost: 1.8362062 error_rate: 0.78\n",
      "i: 172 j: 347 nb: 348 cost: 1.8341279 error_rate: 0.78\n",
      "i: 173 j: 347 nb: 348 cost: 1.8345035 error_rate: 0.78\n",
      "i: 174 j: 347 nb: 348 cost: 1.8330004 error_rate: 0.78\n",
      "i: 175 j: 347 nb: 348 cost: 1.8341238 error_rate: 0.78\n",
      "i: 176 j: 347 nb: 348 cost: 1.8337448 error_rate: 0.78\n",
      "i: 177 j: 347 nb: 348 cost: 1.8324144 error_rate: 0.78\n",
      "i: 178 j: 347 nb: 348 cost: 1.8345542 error_rate: 0.78\n",
      "i: 179 j: 347 nb: 348 cost: 1.8347877 error_rate: 0.78\n",
      "i: 180 j: 347 nb: 348 cost: 1.8335451 error_rate: 0.78\n",
      "i: 181 j: 347 nb: 348 cost: 1.8384569 error_rate: 0.78\n",
      "i: 182 j: 347 nb: 348 cost: 1.8350748 error_rate: 0.78\n",
      "i: 183 j: 347 nb: 348 cost: 1.8381636 error_rate: 0.78\n",
      "i: 184 j: 347 nb: 348 cost: 1.8442484 error_rate: 0.78\n",
      "i: 185 j: 347 nb: 348 cost: 1.836532 error_rate: 0.78\n",
      "i: 186 j: 347 nb: 348 cost: 1.83622 error_rate: 0.78\n",
      "i: 187 j: 347 nb: 348 cost: 1.8361318 error_rate: 0.78\n",
      "i: 188 j: 347 nb: 348 cost: 1.8336674 error_rate: 0.78\n",
      "i: 189 j: 347 nb: 348 cost: 1.8377074 error_rate: 0.78\n",
      "i: 190 j: 347 nb: 348 cost: 1.8361024 error_rate: 0.78\n",
      "i: 191 j: 347 nb: 348 cost: 1.8345675 error_rate: 0.78\n",
      "i: 192 j: 347 nb: 348 cost: 1.8345989 error_rate: 0.78\n",
      "i: 193 j: 347 nb: 348 cost: 1.8318297 error_rate: 0.78\n",
      "i: 194 j: 347 nb: 348 cost: 1.8332037 error_rate: 0.78\n",
      "i: 195 j: 347 nb: 348 cost: 1.840379 error_rate: 0.78\n",
      "i: 196 j: 347 nb: 348 cost: 1.8362489 error_rate: 0.78\n",
      "i: 197 j: 347 nb: 348 cost: 1.834914 error_rate: 0.78\n",
      "i: 198 j: 347 nb: 348 cost: 1.8367774 error_rate: 0.78\n",
      "i: 199 j: 347 nb: 348 cost: 1.846755 error_rate: 0.78\n",
      "i: 200 j: 347 nb: 348 cost: 1.8415532 error_rate: 0.78\n",
      "i: 201 j: 347 nb: 348 cost: 1.8389534 error_rate: 0.78\n",
      "i: 202 j: 347 nb: 348 cost: 1.8485359 error_rate: 0.78\n",
      "i: 203 j: 347 nb: 348 cost: 1.8352557 error_rate: 0.78\n",
      "i: 204 j: 347 nb: 348 cost: 1.8364329 error_rate: 0.78\n",
      "i: 205 j: 347 nb: 348 cost: 1.8349234 error_rate: 0.78\n",
      "i: 206 j: 347 nb: 348 cost: 1.8322514 error_rate: 0.78\n",
      "i: 207 j: 347 nb: 348 cost: 1.8327447 error_rate: 0.78\n",
      "i: 208 j: 347 nb: 348 cost: 1.8345213 error_rate: 0.78\n",
      "i: 209 j: 347 nb: 348 cost: 1.8322788 error_rate: 0.78\n",
      "i: 210 j: 347 nb: 348 cost: 1.8370769 error_rate: 0.78\n",
      "i: 211 j: 347 nb: 348 cost: 1.8367988 error_rate: 0.78\n",
      "i: 212 j: 347 nb: 348 cost: 1.8383378 error_rate: 0.78\n",
      "i: 213 j: 347 nb: 348 cost: 1.8362429 error_rate: 0.78\n",
      "i: 214 j: 347 nb: 348 cost: 1.8323014 error_rate: 0.78\n",
      "i: 215 j: 347 nb: 348 cost: 1.8363898 error_rate: 0.78\n",
      "i: 216 j: 347 nb: 348 cost: 1.8404016 error_rate: 0.78\n",
      "i: 217 j: 347 nb: 348 cost: 1.8590925 error_rate: 0.78\n",
      "i: 218 j: 347 nb: 348 cost: 1.8412075 error_rate: 0.78\n",
      "i: 219 j: 347 nb: 348 cost: 1.8373227 error_rate: 0.78\n",
      "i: 220 j: 347 nb: 348 cost: 1.8356298 error_rate: 0.78\n",
      "i: 221 j: 347 nb: 348 cost: 1.8344868 error_rate: 0.78\n",
      "i: 222 j: 347 nb: 348 cost: 1.8346413 error_rate: 0.78\n",
      "i: 223 j: 347 nb: 348 cost: 1.8366565 error_rate: 0.78\n",
      "i: 224 j: 347 nb: 348 cost: 1.8415743 error_rate: 0.78\n",
      "i: 225 j: 347 nb: 348 cost: 1.8346782 error_rate: 0.78\n",
      "i: 226 j: 347 nb: 348 cost: 1.8373398 error_rate: 0.78\n",
      "i: 227 j: 347 nb: 348 cost: 1.8354402 error_rate: 0.78\n",
      "i: 228 j: 347 nb: 348 cost: 1.8325539 error_rate: 0.78\n",
      "i: 229 j: 347 nb: 348 cost: 1.8374434 error_rate: 0.78\n",
      "i: 230 j: 347 nb: 348 cost: 1.8345414 error_rate: 0.78\n",
      "i: 231 j: 347 nb: 348 cost: 1.835703 error_rate: 0.78\n",
      "i: 232 j: 347 nb: 348 cost: 1.8397244 error_rate: 0.78\n",
      "i: 233 j: 347 nb: 348 cost: 1.8407567 error_rate: 0.78\n",
      "i: 234 j: 347 nb: 348 cost: 1.8353425 error_rate: 0.78\n",
      "i: 235 j: 347 nb: 348 cost: 1.8343246 error_rate: 0.78\n",
      "i: 236 j: 347 nb: 348 cost: 1.8323408 error_rate: 0.78\n",
      "i: 237 j: 347 nb: 348 cost: 1.8337387 error_rate: 0.78\n",
      "i: 238 j: 347 nb: 348 cost: 1.8414559 error_rate: 0.78\n",
      "i: 239 j: 347 nb: 348 cost: 1.8352662 error_rate: 0.78\n",
      "i: 240 j: 347 nb: 348 cost: 1.8467131 error_rate: 0.78\n",
      "i: 241 j: 347 nb: 348 cost: 1.8364767 error_rate: 0.78\n",
      "i: 242 j: 347 nb: 348 cost: 1.8425465 error_rate: 0.78\n",
      "i: 243 j: 347 nb: 348 cost: 1.8404377 error_rate: 0.78\n",
      "i: 244 j: 347 nb: 348 cost: 1.8415718 error_rate: 0.78\n",
      "i: 245 j: 347 nb: 348 cost: 1.8419083 error_rate: 0.78\n",
      "i: 246 j: 347 nb: 348 cost: 1.8401895 error_rate: 0.78\n",
      "i: 247 j: 347 nb: 348 cost: 1.8408295 error_rate: 0.78\n",
      "i: 248 j: 347 nb: 348 cost: 1.8359811 error_rate: 0.78\n",
      "i: 249 j: 347 nb: 348 cost: 1.8352396 error_rate: 0.78\n",
      "i: 250 j: 347 nb: 348 cost: 1.8371346 error_rate: 0.78\n",
      "i: 251 j: 347 nb: 348 cost: 1.8341773 error_rate: 0.78\n",
      "i: 252 j: 347 nb: 348 cost: 1.8334937 error_rate: 0.78\n",
      "i: 253 j: 347 nb: 348 cost: 1.8338735 error_rate: 0.78\n",
      "i: 254 j: 347 nb: 348 cost: 1.833658 error_rate: 0.78\n",
      "i: 255 j: 347 nb: 348 cost: 1.8321015 error_rate: 0.78\n",
      "i: 256 j: 347 nb: 348 cost: 1.8328004 error_rate: 0.78\n",
      "i: 257 j: 347 nb: 348 cost: 1.8336267 error_rate: 0.78\n",
      "i: 258 j: 347 nb: 348 cost: 1.8356355 error_rate: 0.78\n",
      "i: 259 j: 347 nb: 348 cost: 1.837248 error_rate: 0.78\n",
      "i: 260 j: 347 nb: 348 cost: 1.8370935 error_rate: 0.78\n",
      "i: 261 j: 347 nb: 348 cost: 1.8424543 error_rate: 0.78\n",
      "i: 262 j: 347 nb: 348 cost: 1.8378413 error_rate: 0.78\n",
      "i: 263 j: 347 nb: 348 cost: 1.8363916 error_rate: 0.78\n",
      "i: 264 j: 347 nb: 348 cost: 1.8363228 error_rate: 0.78\n",
      "i: 265 j: 347 nb: 348 cost: 1.8352339 error_rate: 0.78\n",
      "i: 266 j: 347 nb: 348 cost: 1.8381933 error_rate: 0.78\n",
      "i: 267 j: 347 nb: 348 cost: 1.83408 error_rate: 0.78\n",
      "i: 268 j: 347 nb: 348 cost: 1.8329116 error_rate: 0.78\n",
      "i: 269 j: 347 nb: 348 cost: 1.8328586 error_rate: 0.78\n",
      "i: 270 j: 347 nb: 348 cost: 1.8379884 error_rate: 0.78\n",
      "i: 271 j: 347 nb: 348 cost: 1.8355372 error_rate: 0.78\n",
      "i: 272 j: 347 nb: 348 cost: 1.8380694 error_rate: 0.78\n",
      "i: 273 j: 347 nb: 348 cost: 1.8385502 error_rate: 0.78\n",
      "i: 274 j: 347 nb: 348 cost: 1.8366832 error_rate: 0.78\n",
      "i: 275 j: 347 nb: 348 cost: 1.8363007 error_rate: 0.78\n",
      "i: 276 j: 347 nb: 348 cost: 1.8349608 error_rate: 0.78\n",
      "i: 277 j: 347 nb: 348 cost: 1.835578 error_rate: 0.78\n",
      "i: 278 j: 347 nb: 348 cost: 1.8346586 error_rate: 0.78\n",
      "i: 279 j: 347 nb: 348 cost: 1.8349199 error_rate: 0.78\n",
      "i: 280 j: 347 nb: 348 cost: 1.8328328 error_rate: 0.78\n",
      "i: 281 j: 347 nb: 348 cost: 1.8329822 error_rate: 0.78\n",
      "i: 282 j: 347 nb: 348 cost: 1.8332372 error_rate: 0.78\n",
      "i: 283 j: 347 nb: 348 cost: 1.8347821 error_rate: 0.78\n",
      "i: 284 j: 347 nb: 348 cost: 1.8371204 error_rate: 0.78\n",
      "i: 285 j: 347 nb: 348 cost: 1.8381212 error_rate: 0.78\n",
      "i: 286 j: 347 nb: 348 cost: 1.8501848 error_rate: 0.78\n",
      "i: 287 j: 347 nb: 348 cost: 1.8390931 error_rate: 0.78\n",
      "i: 288 j: 347 nb: 348 cost: 1.8391513 error_rate: 0.78\n",
      "i: 289 j: 347 nb: 348 cost: 1.834863 error_rate: 0.78\n",
      "i: 290 j: 347 nb: 348 cost: 1.8333908 error_rate: 0.78\n",
      "i: 291 j: 347 nb: 348 cost: 1.833497 error_rate: 0.78\n",
      "i: 292 j: 347 nb: 348 cost: 1.8324808 error_rate: 0.78\n",
      "i: 293 j: 347 nb: 348 cost: 1.8325651 error_rate: 0.78\n",
      "i: 294 j: 347 nb: 348 cost: 1.8378803 error_rate: 0.78\n",
      "i: 295 j: 347 nb: 348 cost: 1.8338262 error_rate: 0.78\n",
      "i: 296 j: 347 nb: 348 cost: 1.8345917 error_rate: 0.78\n",
      "i: 297 j: 347 nb: 348 cost: 1.8346226 error_rate: 0.78\n",
      "i: 298 j: 347 nb: 348 cost: 1.8331958 error_rate: 0.78\n",
      "i: 299 j: 347 nb: 348 cost: 1.8344285 error_rate: 0.78\n",
      "i: 300 j: 347 nb: 348 cost: 1.8342156 error_rate: 0.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 301 j: 347 nb: 348 cost: 1.8378541 error_rate: 0.78\n",
      "i: 302 j: 347 nb: 348 cost: 1.8350276 error_rate: 0.78\n",
      "i: 303 j: 347 nb: 348 cost: 1.8400712 error_rate: 0.78\n",
      "i: 304 j: 347 nb: 348 cost: 1.8332812 error_rate: 0.78\n",
      "i: 305 j: 347 nb: 348 cost: 1.8397777 error_rate: 0.78\n",
      "i: 306 j: 347 nb: 348 cost: 1.8637946 error_rate: 0.78\n",
      "i: 307 j: 347 nb: 348 cost: 1.8464848 error_rate: 0.78\n",
      "i: 308 j: 347 nb: 348 cost: 1.8443048 error_rate: 0.78\n",
      "i: 309 j: 347 nb: 348 cost: 1.8435733 error_rate: 0.78\n",
      "i: 310 j: 347 nb: 348 cost: 1.8367893 error_rate: 0.78\n",
      "i: 311 j: 347 nb: 348 cost: 1.8384717 error_rate: 0.78\n",
      "i: 312 j: 347 nb: 348 cost: 1.8362802 error_rate: 0.78\n",
      "i: 313 j: 347 nb: 348 cost: 1.8337195 error_rate: 0.78\n",
      "i: 314 j: 347 nb: 348 cost: 1.8341062 error_rate: 0.78\n",
      "i: 315 j: 347 nb: 348 cost: 1.8330044 error_rate: 0.78\n",
      "i: 316 j: 347 nb: 348 cost: 1.8378125 error_rate: 0.78\n",
      "i: 317 j: 347 nb: 348 cost: 1.8353584 error_rate: 0.78\n",
      "i: 318 j: 347 nb: 348 cost: 1.8367152 error_rate: 0.78\n",
      "i: 319 j: 347 nb: 348 cost: 1.8368875 error_rate: 0.78\n",
      "i: 320 j: 347 nb: 348 cost: 1.8365626 error_rate: 0.78\n",
      "i: 321 j: 347 nb: 348 cost: 1.841083 error_rate: 0.78\n",
      "i: 322 j: 347 nb: 348 cost: 1.8389431 error_rate: 0.78\n",
      "i: 323 j: 347 nb: 348 cost: 1.8370478 error_rate: 0.78\n",
      "i: 324 j: 347 nb: 348 cost: 1.832595 error_rate: 0.78\n",
      "i: 325 j: 347 nb: 348 cost: 1.8344495 error_rate: 0.78\n",
      "i: 326 j: 347 nb: 348 cost: 1.8357391 error_rate: 0.78\n",
      "i: 327 j: 347 nb: 348 cost: 1.8345364 error_rate: 0.78\n",
      "i: 328 j: 347 nb: 348 cost: 1.836778 error_rate: 0.78\n",
      "i: 329 j: 347 nb: 348 cost: 1.8372699 error_rate: 0.78\n",
      "i: 330 j: 347 nb: 348 cost: 1.8331864 error_rate: 0.78\n",
      "i: 331 j: 347 nb: 348 cost: 1.8412411 error_rate: 0.78\n",
      "i: 332 j: 347 nb: 348 cost: 1.8368993 error_rate: 0.78\n",
      "i: 333 j: 347 nb: 348 cost: 1.8347198 error_rate: 0.78\n",
      "i: 334 j: 347 nb: 348 cost: 1.8359705 error_rate: 0.78\n",
      "i: 335 j: 347 nb: 348 cost: 1.8360143 error_rate: 0.78\n",
      "i: 336 j: 347 nb: 348 cost: 1.8341515 error_rate: 0.78\n",
      "i: 337 j: 347 nb: 348 cost: 1.834946 error_rate: 0.78\n",
      "i: 338 j: 347 nb: 348 cost: 1.8544077 error_rate: 0.78\n",
      "i: 339 j: 347 nb: 348 cost: 1.8411405 error_rate: 0.78\n",
      "i: 340 j: 347 nb: 348 cost: 1.8370844 error_rate: 0.78\n",
      "i: 341 j: 347 nb: 348 cost: 1.8338898 error_rate: 0.78\n",
      "i: 342 j: 347 nb: 348 cost: 1.83551 error_rate: 0.78\n",
      "i: 343 j: 347 nb: 348 cost: 1.8370565 error_rate: 0.78\n",
      "i: 344 j: 347 nb: 348 cost: 1.8384695 error_rate: 0.78\n",
      "i: 345 j: 347 nb: 348 cost: 1.8368379 error_rate: 0.78\n",
      "i: 346 j: 347 nb: 348 cost: 1.8403503 error_rate: 0.78\n",
      "i: 347 j: 347 nb: 348 cost: 1.8353279 error_rate: 0.78\n",
      "i: 348 j: 347 nb: 348 cost: 1.8392863 error_rate: 0.78\n",
      "i: 349 j: 347 nb: 348 cost: 1.8360928 error_rate: 0.78\n",
      "i: 350 j: 347 nb: 348 cost: 1.8355936 error_rate: 0.78\n",
      "i: 351 j: 347 nb: 348 cost: 1.8368598 error_rate: 0.78\n",
      "i: 352 j: 347 nb: 348 cost: 1.835432 error_rate: 0.78\n",
      "i: 353 j: 347 nb: 348 cost: 1.8376453 error_rate: 0.78\n",
      "i: 354 j: 347 nb: 348 cost: 1.8515095 error_rate: 0.78\n",
      "i: 355 j: 347 nb: 348 cost: 1.8385475 error_rate: 0.78\n",
      "i: 356 j: 347 nb: 348 cost: 1.8413527 error_rate: 0.78\n",
      "i: 357 j: 347 nb: 348 cost: 1.8445337 error_rate: 0.78\n",
      "i: 358 j: 347 nb: 348 cost: 1.8386512 error_rate: 0.78\n",
      "i: 359 j: 347 nb: 348 cost: 1.8353523 error_rate: 0.78\n",
      "i: 360 j: 347 nb: 348 cost: 1.8325055 error_rate: 0.78\n",
      "i: 361 j: 347 nb: 348 cost: 1.8345515 error_rate: 0.78\n",
      "i: 362 j: 347 nb: 348 cost: 1.8349708 error_rate: 0.78\n",
      "i: 363 j: 347 nb: 348 cost: 1.8522636 error_rate: 0.78\n",
      "i: 364 j: 347 nb: 348 cost: 1.8381077 error_rate: 0.78\n",
      "i: 365 j: 347 nb: 348 cost: 1.8384184 error_rate: 0.78\n",
      "i: 366 j: 347 nb: 348 cost: 1.834524 error_rate: 0.78\n",
      "i: 367 j: 347 nb: 348 cost: 1.835305 error_rate: 0.78\n",
      "i: 368 j: 347 nb: 348 cost: 1.8348445 error_rate: 0.78\n",
      "i: 369 j: 347 nb: 348 cost: 1.8356647 error_rate: 0.78\n",
      "i: 370 j: 347 nb: 348 cost: 1.8380098 error_rate: 0.78\n",
      "i: 371 j: 347 nb: 348 cost: 1.833326 error_rate: 0.78\n",
      "i: 372 j: 347 nb: 348 cost: 1.8331025 error_rate: 0.78\n",
      "i: 373 j: 347 nb: 348 cost: 1.8356576 error_rate: 0.78\n",
      "i: 374 j: 347 nb: 348 cost: 1.8360316 error_rate: 0.78\n",
      "i: 375 j: 347 nb: 348 cost: 1.8363459 error_rate: 0.78\n",
      "i: 376 j: 347 nb: 348 cost: 1.83499 error_rate: 0.78\n",
      "i: 377 j: 347 nb: 348 cost: 1.8355602 error_rate: 0.78\n",
      "i: 378 j: 347 nb: 348 cost: 1.8330731 error_rate: 0.78\n",
      "i: 379 j: 347 nb: 348 cost: 1.8361143 error_rate: 0.78\n",
      "i: 380 j: 347 nb: 348 cost: 1.834027 error_rate: 0.78\n",
      "i: 381 j: 347 nb: 348 cost: 1.8333353 error_rate: 0.78\n",
      "i: 382 j: 347 nb: 348 cost: 1.8362037 error_rate: 0.78\n",
      "i: 383 j: 347 nb: 348 cost: 1.83412 error_rate: 0.78\n",
      "i: 384 j: 347 nb: 348 cost: 1.8377197 error_rate: 0.78\n",
      "i: 385 j: 347 nb: 348 cost: 1.840241 error_rate: 0.78\n",
      "i: 386 j: 347 nb: 348 cost: 1.8440437 error_rate: 0.78\n",
      "i: 387 j: 347 nb: 348 cost: 1.838113 error_rate: 0.78\n",
      "i: 388 j: 347 nb: 348 cost: 1.8431791 error_rate: 0.78\n",
      "i: 389 j: 347 nb: 348 cost: 1.8385878 error_rate: 0.78\n",
      "i: 390 j: 347 nb: 348 cost: 1.8363253 error_rate: 0.78\n",
      "i: 391 j: 347 nb: 348 cost: 1.8356687 error_rate: 0.78\n",
      "i: 392 j: 347 nb: 348 cost: 1.8337657 error_rate: 0.78\n",
      "i: 393 j: 347 nb: 348 cost: 1.8358887 error_rate: 0.78\n",
      "i: 394 j: 347 nb: 348 cost: 1.8348867 error_rate: 0.78\n",
      "i: 395 j: 347 nb: 348 cost: 1.836185 error_rate: 0.78\n",
      "i: 396 j: 347 nb: 348 cost: 1.8337823 error_rate: 0.78\n",
      "i: 397 j: 347 nb: 348 cost: 1.8334676 error_rate: 0.78\n",
      "i: 398 j: 347 nb: 348 cost: 1.8348355 error_rate: 0.78\n",
      "i: 399 j: 347 nb: 348 cost: 1.836091 error_rate: 0.78\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWYdwoJuZdQgHuplZh/j/o5DJsHPea5kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ac1a398ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ANN([2000, 1000, 500])\n",
    "model.fit(X, Y, show_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'b' of 'MatMul' Op has type float32 that does not match type float64 of argument 'a'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    511\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[0;32m    925\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[1;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[1;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m         (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[0;32m    775\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor conversion requested dtype float64 for Tensor with dtype float32: 'Tensor(\"Variable_48/read:0\", shape=(2304, 2000), dtype=float32)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-4bc6f9720928>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-1138488583b7>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-1138488583b7>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-1138488583b7>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mANN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   1889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[1;32m-> 1891\u001b[1;33m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[0;32m   1892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m_mat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   2434\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m   2435\u001b[0m         \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2436\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m   2437\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2438\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    544\u001b[0m                   \u001b[1;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[1;32m--> 546\u001b[1;33m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Input 'b' of 'MatMul' Op has type float32 that does not match type float64 of argument 'a'."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(Y[:-500], model.predict(X[:-500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 2 but is rank 1 for 'MatMul_51' (op: 'MatMul') with input shapes: [2304], [2304,2000].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    687\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Shape must be rank 2 but is rank 1 for 'MatMul_51' (op: 'MatMul') with input shapes: [2304], [2304,2000].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-aacd541697d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-1138488583b7>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-1138488583b7>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-1138488583b7>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mANN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   1889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[1;32m-> 1891\u001b[1;33m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[0;32m   1892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m_mat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   2434\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m   2435\u001b[0m         \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2436\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m   2437\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2438\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    788\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2956\u001b[0m         op_def=op_def)\n\u001b[0;32m   2957\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2958\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2209\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2210\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   2157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2158\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2159\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, require_shape_fn)\u001b[0m\n\u001b[0;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m                                   require_shape_fn)\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape must be rank 2 but is rank 1 for 'MatMul_51' (op: 'MatMul') with input shapes: [2304], [2304,2000]."
     ]
    }
   ],
   "source": [
    "model.predict(X[-1].astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[-1].astype(np.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
