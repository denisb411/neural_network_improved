{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denis\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you have to specify the type\n",
    "A = tf.placeholder(tf.float32, shape=(5, 5), name='A')\n",
    "\n",
    "\n",
    "# but shape and name are optional\n",
    "v = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# I think this name is more appropriate than 'dot'\n",
    "w = tf.matmul(A, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00472598]\n",
      " [-0.43340614]\n",
      " [ 0.05661434]\n",
      " [-0.58585685]\n",
      " [-0.39839315]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# similar to Theano, you need to \"feed\" the variables values.\n",
    "# In TensorFlow you do the \"actual work\" in a \"session\".\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # the values are fed in via the appropriately named argument \"feed_dict\"\n",
    "    # v needs to be of shape=(5, 1) not just shape=(5,)\n",
    "    # it's more like \"real\" matrix multiplication\n",
    "    output = session.run(w, feed_dict={A: np.random.randn(5, 5), v: np.random.randn(5, 1)})\n",
    "\n",
    "    # what's this output that is returned by the session? let's print it\n",
    "    print(output, type(output))\n",
    "    \n",
    "    # luckily, the output type is just a numpy array. back to safety!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A tf variable can be initialized with a numpy array or a tf array\n",
    "# or more correctly, anything that can be turned into a tf tensor\n",
    "shape = (2, 2)\n",
    "x = tf.Variable(tf.random_normal(shape))\n",
    "# x = tf.Variable(np.random.randn(2, 2))\n",
    "t = tf.Variable(0) # a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you need to \"initialize\" the variables first\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[[-1.0526377   0.4078191 ]\n",
      " [-0.66109604 -1.5805446 ]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    out = session.run(init) # and then \"run\" the init operation\n",
    "    print(out) # it's just None\n",
    "\n",
    "    # eval() in tf is like get_value() in Theano\n",
    "    print(x.eval()) # the initial value of x\n",
    "    print(t.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now try to find the minimum of a simple cost function like we did in Theano\n",
    "u = tf.Variable(20.0)\n",
    "cost = u*u + u + 1.0\n",
    "\n",
    "# One difference between Theano and TensorFlow is that you don't write the updates\n",
    "# yourself in TensorFlow. You choose an optimizer that implements the algorithm you want.\n",
    "# 0.3 is the learning rate. Documentation lists the params.\n",
    "train_op = tf.train.GradientDescentOptimizer(0.3).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0, cost = 67.990, u = 7.700\n",
      "i = 1, cost = 11.508, u = 2.780\n",
      "i = 2, cost = 2.471, u = 0.812\n",
      "i = 3, cost = 1.025, u = 0.025\n",
      "i = 4, cost = 0.794, u = -0.290\n",
      "i = 5, cost = 0.757, u = -0.416\n",
      "i = 6, cost = 0.751, u = -0.466\n",
      "i = 7, cost = 0.750, u = -0.487\n",
      "i = 8, cost = 0.750, u = -0.495\n",
      "i = 9, cost = 0.750, u = -0.498\n",
      "i = 10, cost = 0.750, u = -0.499\n",
      "i = 11, cost = 0.750, u = -0.500\n"
     ]
    }
   ],
   "source": [
    "# let's run a session again\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "\n",
    "    # Strangely, while the weight update is automated, the loop itself is not.\n",
    "    # So we'll just call train_op until convergence.\n",
    "    # This is useful for us anyway since we want to track the cost function.\n",
    "    for i in range(12):\n",
    "        session.run(train_op)\n",
    "        print(\"i = %d, cost = %.3f, u = %.3f\" % (i, cost.eval(), u.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_rate(p, t):\n",
    "    return np.mean(p != t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in and transforming data...\n"
     ]
    }
   ],
   "source": [
    "from util import get_normalized_data, y2indicator\n",
    "\n",
    "# step 1: get the data and define all the usual variables\n",
    "X, Y = get_normalized_data()\n",
    "\n",
    "max_iter = 15\n",
    "print_period = 10\n",
    "\n",
    "lr = 0.00004\n",
    "reg = 0.01\n",
    "\n",
    "Xtrain = X[:-1000,]\n",
    "Ytrain = Y[:-1000]\n",
    "Xtest  = X[-1000:,]\n",
    "Ytest  = Y[-1000:]\n",
    "Ytrain_ind = y2indicator(Ytrain)\n",
    "Ytest_ind = y2indicator(Ytest)\n",
    "\n",
    "N, D = Xtrain.shape\n",
    "batch_sz = 500\n",
    "n_batches = N // batch_sz\n",
    "\n",
    "# add an extra layer just for fun\n",
    "M1 = 300\n",
    "M2 = 100\n",
    "K = 10\n",
    "W1_init = np.random.randn(D, M1) / 28\n",
    "b1_init = np.zeros(M1)\n",
    "W2_init = np.random.randn(M1, M2) / np.sqrt(M1)\n",
    "b2_init = np.zeros(M2)\n",
    "W3_init = np.random.randn(M2, K) / np.sqrt(M2)\n",
    "b3_init = np.zeros(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define variables and expressions\n",
    "X = tf.placeholder(tf.float32, shape=(None, D), name='X')\n",
    "T = tf.placeholder(tf.float32, shape=(None, K), name='T')\n",
    "W1 = tf.Variable(W1_init.astype(np.float32))\n",
    "b1 = tf.Variable(b1_init.astype(np.float32))\n",
    "W2 = tf.Variable(W2_init.astype(np.float32))\n",
    "b2 = tf.Variable(b2_init.astype(np.float32))\n",
    "W3 = tf.Variable(W3_init.astype(np.float32))\n",
    "b3 = tf.Variable(b3_init.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "Z1 = tf.nn.relu( tf.matmul(X, W1) + b1 )\n",
    "Z2 = tf.nn.relu( tf.matmul(Z1, W2) + b2 )\n",
    "Yish = tf.matmul(Z2, W3) + b3 # remember, the cost function does the softmaxing! weird, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# softmax_cross_entropy_with_logits take in the \"logits\"\n",
    "# if you wanted to know the actual output of the neural net,\n",
    "# you could pass \"Yish\" into tf.nn.softmax(logits)\n",
    "cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=Yish, labels=T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we choose the optimizer but don't implement the algorithm ourselves\n",
    "# let's go with RMSprop, since we just learned about it.\n",
    "# it includes momentum!\n",
    "train_op = tf.train.RMSPropOptimizer(lr, decay=0.99, momentum=0.9).minimize(cost)\n",
    "\n",
    "# we'll use this to calculate the error rate\n",
    "predict_op = tf.argmax(Yish, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost / err at iteration i=0, j=0: 2411.788 / 0.921\n",
      "Cost / err at iteration i=0, j=10: 1586.887 / 0.351\n",
      "Cost / err at iteration i=0, j=20: 881.479 / 0.218\n",
      "Cost / err at iteration i=0, j=30: 552.248 / 0.150\n",
      "Cost / err at iteration i=0, j=40: 418.609 / 0.121\n",
      "Cost / err at iteration i=0, j=50: 354.205 / 0.108\n",
      "Cost / err at iteration i=0, j=60: 309.814 / 0.094\n",
      "Cost / err at iteration i=0, j=70: 281.038 / 0.084\n",
      "Cost / err at iteration i=0, j=80: 258.173 / 0.072\n",
      "Cost / err at iteration i=1, j=0: 253.879 / 0.071\n",
      "Cost / err at iteration i=1, j=10: 238.339 / 0.067\n",
      "Cost / err at iteration i=1, j=20: 226.287 / 0.065\n",
      "Cost / err at iteration i=1, j=30: 213.087 / 0.061\n",
      "Cost / err at iteration i=1, j=40: 202.742 / 0.059\n",
      "Cost / err at iteration i=1, j=50: 194.987 / 0.058\n",
      "Cost / err at iteration i=1, j=60: 185.012 / 0.056\n",
      "Cost / err at iteration i=1, j=70: 176.488 / 0.054\n",
      "Cost / err at iteration i=1, j=80: 168.628 / 0.047\n",
      "Cost / err at iteration i=2, j=0: 166.704 / 0.045\n",
      "Cost / err at iteration i=2, j=10: 162.353 / 0.043\n",
      "Cost / err at iteration i=2, j=20: 159.001 / 0.046\n",
      "Cost / err at iteration i=2, j=30: 154.846 / 0.043\n",
      "Cost / err at iteration i=2, j=40: 151.354 / 0.044\n",
      "Cost / err at iteration i=2, j=50: 147.821 / 0.046\n",
      "Cost / err at iteration i=2, j=60: 143.673 / 0.044\n",
      "Cost / err at iteration i=2, j=70: 135.168 / 0.038\n",
      "Cost / err at iteration i=2, j=80: 130.478 / 0.037\n",
      "Cost / err at iteration i=3, j=0: 128.737 / 0.039\n",
      "Cost / err at iteration i=3, j=10: 125.923 / 0.036\n",
      "Cost / err at iteration i=3, j=20: 125.647 / 0.034\n",
      "Cost / err at iteration i=3, j=30: 125.144 / 0.033\n",
      "Cost / err at iteration i=3, j=40: 122.897 / 0.037\n",
      "Cost / err at iteration i=3, j=50: 121.805 / 0.036\n",
      "Cost / err at iteration i=3, j=60: 118.828 / 0.033\n",
      "Cost / err at iteration i=3, j=70: 110.908 / 0.031\n",
      "Cost / err at iteration i=3, j=80: 107.584 / 0.029\n",
      "Cost / err at iteration i=4, j=0: 106.167 / 0.031\n",
      "Cost / err at iteration i=4, j=10: 105.211 / 0.029\n",
      "Cost / err at iteration i=4, j=20: 106.285 / 0.029\n",
      "Cost / err at iteration i=4, j=30: 106.794 / 0.030\n",
      "Cost / err at iteration i=4, j=40: 104.772 / 0.030\n",
      "Cost / err at iteration i=4, j=50: 105.293 / 0.032\n",
      "Cost / err at iteration i=4, j=60: 103.278 / 0.029\n",
      "Cost / err at iteration i=4, j=70: 95.751 / 0.027\n",
      "Cost / err at iteration i=4, j=80: 93.249 / 0.025\n",
      "Cost / err at iteration i=5, j=0: 91.993 / 0.024\n",
      "Cost / err at iteration i=5, j=10: 92.647 / 0.028\n",
      "Cost / err at iteration i=5, j=20: 94.129 / 0.025\n",
      "Cost / err at iteration i=5, j=30: 95.415 / 0.026\n",
      "Cost / err at iteration i=5, j=40: 94.193 / 0.026\n",
      "Cost / err at iteration i=5, j=50: 95.270 / 0.025\n",
      "Cost / err at iteration i=5, j=60: 93.545 / 0.024\n",
      "Cost / err at iteration i=5, j=70: 87.210 / 0.027\n",
      "Cost / err at iteration i=5, j=80: 85.303 / 0.022\n",
      "Cost / err at iteration i=6, j=0: 84.151 / 0.021\n",
      "Cost / err at iteration i=6, j=10: 85.245 / 0.027\n",
      "Cost / err at iteration i=6, j=20: 85.293 / 0.022\n",
      "Cost / err at iteration i=6, j=30: 87.270 / 0.024\n",
      "Cost / err at iteration i=6, j=40: 85.721 / 0.020\n",
      "Cost / err at iteration i=6, j=50: 87.981 / 0.023\n",
      "Cost / err at iteration i=6, j=60: 86.701 / 0.024\n",
      "Cost / err at iteration i=6, j=70: 81.335 / 0.024\n",
      "Cost / err at iteration i=6, j=80: 80.418 / 0.021\n",
      "Cost / err at iteration i=7, j=0: 79.423 / 0.021\n",
      "Cost / err at iteration i=7, j=10: 81.632 / 0.026\n",
      "Cost / err at iteration i=7, j=20: 81.233 / 0.024\n",
      "Cost / err at iteration i=7, j=30: 82.551 / 0.021\n",
      "Cost / err at iteration i=7, j=40: 81.530 / 0.019\n",
      "Cost / err at iteration i=7, j=50: 84.906 / 0.022\n",
      "Cost / err at iteration i=7, j=60: 83.793 / 0.021\n",
      "Cost / err at iteration i=7, j=70: 78.385 / 0.023\n",
      "Cost / err at iteration i=7, j=80: 76.661 / 0.022\n",
      "Cost / err at iteration i=8, j=0: 75.626 / 0.022\n",
      "Cost / err at iteration i=8, j=10: 77.645 / 0.024\n",
      "Cost / err at iteration i=8, j=20: 78.642 / 0.025\n",
      "Cost / err at iteration i=8, j=30: 79.386 / 0.019\n",
      "Cost / err at iteration i=8, j=40: 78.384 / 0.020\n",
      "Cost / err at iteration i=8, j=50: 82.822 / 0.023\n",
      "Cost / err at iteration i=8, j=60: 82.053 / 0.020\n",
      "Cost / err at iteration i=8, j=70: 76.773 / 0.022\n",
      "Cost / err at iteration i=8, j=80: 74.863 / 0.022\n",
      "Cost / err at iteration i=9, j=0: 73.729 / 0.022\n",
      "Cost / err at iteration i=9, j=10: 75.104 / 0.021\n",
      "Cost / err at iteration i=9, j=20: 77.563 / 0.023\n",
      "Cost / err at iteration i=9, j=30: 78.226 / 0.021\n",
      "Cost / err at iteration i=9, j=40: 77.805 / 0.020\n",
      "Cost / err at iteration i=9, j=50: 82.512 / 0.024\n",
      "Cost / err at iteration i=9, j=60: 82.193 / 0.021\n",
      "Cost / err at iteration i=9, j=70: 77.464 / 0.022\n",
      "Cost / err at iteration i=9, j=80: 74.565 / 0.021\n",
      "Cost / err at iteration i=10, j=0: 73.352 / 0.022\n",
      "Cost / err at iteration i=10, j=10: 73.694 / 0.020\n",
      "Cost / err at iteration i=10, j=20: 78.159 / 0.024\n",
      "Cost / err at iteration i=10, j=30: 77.436 / 0.020\n",
      "Cost / err at iteration i=10, j=40: 76.201 / 0.020\n",
      "Cost / err at iteration i=10, j=50: 82.660 / 0.023\n",
      "Cost / err at iteration i=10, j=60: 81.889 / 0.021\n",
      "Cost / err at iteration i=10, j=70: 77.992 / 0.022\n",
      "Cost / err at iteration i=10, j=80: 75.019 / 0.021\n",
      "Cost / err at iteration i=11, j=0: 73.821 / 0.021\n",
      "Cost / err at iteration i=11, j=10: 73.422 / 0.020\n",
      "Cost / err at iteration i=11, j=20: 78.755 / 0.024\n",
      "Cost / err at iteration i=11, j=30: 77.349 / 0.020\n",
      "Cost / err at iteration i=11, j=40: 76.598 / 0.020\n",
      "Cost / err at iteration i=11, j=50: 84.579 / 0.023\n",
      "Cost / err at iteration i=11, j=60: 83.160 / 0.021\n",
      "Cost / err at iteration i=11, j=70: 79.326 / 0.023\n",
      "Cost / err at iteration i=11, j=80: 76.056 / 0.023\n",
      "Cost / err at iteration i=12, j=0: 74.638 / 0.023\n",
      "Cost / err at iteration i=12, j=10: 73.558 / 0.020\n",
      "Cost / err at iteration i=12, j=20: 78.942 / 0.024\n",
      "Cost / err at iteration i=12, j=30: 77.099 / 0.022\n",
      "Cost / err at iteration i=12, j=40: 76.568 / 0.019\n",
      "Cost / err at iteration i=12, j=50: 85.135 / 0.023\n",
      "Cost / err at iteration i=12, j=60: 83.247 / 0.021\n",
      "Cost / err at iteration i=12, j=70: 80.309 / 0.022\n",
      "Cost / err at iteration i=12, j=80: 76.849 / 0.023\n",
      "Cost / err at iteration i=13, j=0: 75.356 / 0.023\n",
      "Cost / err at iteration i=13, j=10: 74.717 / 0.022\n",
      "Cost / err at iteration i=13, j=20: 81.285 / 0.023\n",
      "Cost / err at iteration i=13, j=30: 78.776 / 0.021\n",
      "Cost / err at iteration i=13, j=40: 77.499 / 0.018\n",
      "Cost / err at iteration i=13, j=50: 87.823 / 0.021\n",
      "Cost / err at iteration i=13, j=60: 84.285 / 0.021\n",
      "Cost / err at iteration i=13, j=70: 81.462 / 0.023\n",
      "Cost / err at iteration i=13, j=80: 79.302 / 0.022\n",
      "Cost / err at iteration i=14, j=0: 77.503 / 0.022\n",
      "Cost / err at iteration i=14, j=10: 75.579 / 0.022\n",
      "Cost / err at iteration i=14, j=20: 81.565 / 0.022\n",
      "Cost / err at iteration i=14, j=30: 79.164 / 0.020\n",
      "Cost / err at iteration i=14, j=40: 77.431 / 0.019\n",
      "Cost / err at iteration i=14, j=50: 88.547 / 0.023\n",
      "Cost / err at iteration i=14, j=60: 84.844 / 0.021\n",
      "Cost / err at iteration i=14, j=70: 81.947 / 0.023\n",
      "Cost / err at iteration i=14, j=80: 81.269 / 0.022\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-5b0eae3bc284>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mcosts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# increase max_iter and notice how the test cost starts to increase.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "costs = []\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        for j in range(n_batches):\n",
    "            Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "            Ybatch = Ytrain_ind[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "\n",
    "            session.run(train_op, feed_dict={X: Xbatch, T: Ybatch})\n",
    "            if j % print_period == 0:\n",
    "                test_cost = session.run(cost, feed_dict={X: Xtest, T: Ytest_ind})\n",
    "                prediction = session.run(predict_op, feed_dict={X: Xtest})\n",
    "                err = error_rate(prediction, Ytest)\n",
    "                print(\"Cost / err at iteration i=%d, j=%d: %.3f / %.3f\" % (i, j, test_cost, err))\n",
    "                costs.append(test_cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD8CAYAAACYebj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c9eff7bf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(costs)\n",
    "plt.show()\n",
    "# increase max_iter and notice how the test cost starts to increase.\n",
    "# are we overfitting by adding that extra layer?\n",
    "# how would you add regularization to this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
